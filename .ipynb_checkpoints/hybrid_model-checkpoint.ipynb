{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc283e6-664e-476b-977b-4b67e85b2511",
   "metadata": {},
   "source": [
    "### Load required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2974c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames: DataFrame, ncol, nrow\n",
    "using Dates: Dates, DateTime, Time, unix2datetime, Hour, Minute, Microsecond\n",
    "using NativeFileDialog: pick_file\n",
    "using Statistics: median, mean, std\n",
    "using Plots: Plots, plot, plot!, annotate!, hline!, @layout, text, plotly, font, scatter!\n",
    "using Printf: @sprintf\n",
    "using JLD2\n",
    "\n",
    "include(\"model_functions.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481eee05",
   "metadata": {},
   "source": [
    "### Recover earlier separated data from file (Note: does not include Model data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and optimizer states from the JLD2 file\n",
    "current_path = pwd() * \"\\\\Training_data\"\n",
    "filterlist = \"JLD2\"\n",
    "infil = pick_file(current_path; filterlist)\n",
    "\n",
    "# Load all saved data and labels\n",
    "##@load infil training_data training_date training_data_good training_date_good training_data_bad training_date_bad training_indicies_good training_indicies_bad training_labels # median_train std_train\n",
    "@load infil training_data_good training_data_bad training_data_bad\n",
    "\n",
    "println(\"Data and labels loaded successfully.\")\n",
    "println(\"Good data contains \",string(size(training_data_good)[2]),\" records\")\n",
    "println(\"Bad  data contains \",string(size(training_data_bad)[2]),\" records\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b21890",
   "metadata": {},
   "source": [
    "### Build the Model using a hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "\n",
    "# (Keep existing functions: min_max_normalize_matrix, z_score_normalize_matrix, pad_or_truncate, get_heave)\n",
    "\n",
    "function min_max_normalize_matrix(X)\n",
    "####################################\n",
    "    \n",
    "    min_vals = minimum(X, dims=1)  # Compute min for each column\n",
    "    max_vals = maximum(X, dims=1)  # Compute max for each column\n",
    "    \n",
    "    return((X .- min_vals) ./ (max_vals .- min_vals))\n",
    "    \n",
    "end    # min_max_normalize_matrix()\n",
    "\n",
    "\n",
    "function calc_reconstruction_errors(data_matrix, model)\n",
    "#######################################################\n",
    "    \n",
    "    reconstruction_errors = Float32[]\n",
    "    \n",
    "    for record in eachcol(data_matrix)\n",
    "        reconstructed_record = model(record)\n",
    "        error = mean((reconstructed_record .- record).^2)\n",
    "        push!(reconstruction_errors, error)\n",
    "    end\n",
    "    \n",
    "    return(reconstruction_errors)\n",
    "    \n",
    "end    # calc_reconstruction_errors()\n",
    "\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "# Define your refined autoencoder model as you have it\n",
    "refined_model = Chain(\n",
    "    Dense(4608, 256, relu),\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 32, relu),\n",
    "    Dense(32, 128, relu),\n",
    "    Dense(128, 256, relu),\n",
    "    Dense(256, 4608)\n",
    ")\n",
    "\n",
    "# Concatenate and normalize the training data\n",
    "training_data_combined = hcat(training_data_good, training_data_bad)\n",
    "training_data_normalized = min_max_normalize_matrix(training_data_combined)\n",
    "training_data_float32 = Float32.(training_data_normalized)\n",
    "\n",
    "@time begin\n",
    "    \n",
    "    println(\"Building hybrid model now\\n\")\n",
    "    flush(stdout)\n",
    "\n",
    "    # Train the model\n",
    "    loss(x) = Flux.mse(refined_model(x), x)\n",
    "    opt = Adam()\n",
    "    \n",
    "    data = Iterators.repeated((training_data_float32,), 100)\n",
    "    Flux.train!(loss, Flux.params(refined_model), data, opt)\n",
    "    println(\"Model training complete.\")\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254df62a",
   "metadata": {},
   "source": [
    "### Select a .BVA file to check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c069a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Sockets\n",
    "\n",
    "#global X_data = Matrix{Float32}(undef, 0, 0)\n",
    "\n",
    "hostname = gethostname()\n",
    "println(\"The name of the computer is: \", hostname)\n",
    "\n",
    "if hostname == \"QUEENSLAND-BASIN\"\n",
    "    \n",
    "    display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "    initial_path = \"E:\\\\Card Data\\\\\"\n",
    "    \n",
    "else\n",
    "    \n",
    "    display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))    \n",
    "    initial_path = \"F:\\\\Card Data\\\\\"\n",
    "    \n",
    "end\n",
    "\n",
    "REC_LENGTH = 4608       # Number of WSE's in a Mk4 30-minute record\n",
    "SAMPLE_FREQUENCY = 2.56 # Mk4 sample frequency in Hertz\n",
    "SAMPLE_LENGTH = 1800    # record length in seconds\n",
    "SAMPLE_RATE = Float64(1/SAMPLE_FREQUENCY) # sample spacing in seconds\n",
    "\n",
    "#########################################################################################################################\n",
    "##    confidence_interval = 2.576  # corresponds to a 99% confidence interval (for a normal distribution)\n",
    "##    confidence_interval = 3.0    # corresponds to a 99.73% confidence interval (for a normal distribution)    \n",
    "##    confidence_interval = 3.29   # corresponds to a 99.9% confidence interval (for a normal distribution)\n",
    "#########################################################################################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "if hostname == \"QUEENSLAND-BASIN\"\n",
    "    \n",
    "    initial_path = \"E:\\\\Card Data\\\\\"\n",
    "    \n",
    "else\n",
    "    \n",
    "    initial_path = \"F:\\\\Card Data\\\\\"\n",
    "    \n",
    "end\n",
    "\n",
    "infil = pick_file(initial_path)\n",
    "\n",
    "f23_df, Data = get_hex_array(infil)\n",
    "\n",
    "f23_df = get_matches(Data, f23_df)\n",
    "\n",
    "# remove those vectors from F23 df that are not located in the Data vector df\n",
    "f23_df = f23_first_row_check(f23_df)\n",
    "\n",
    "X_data, X_date = get_heave(Data, f23_df);\n",
    "\n",
    "X_data = Float32.(X_data)\n",
    "\n",
    "println(string(length(X_date)),\" records processed.\\n\")\n",
    "println(\"\\nNow run hybrid model against this data to check for outliers!\")\n",
    "flush(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7bdc8b",
   "metadata": {},
   "source": [
    "### Run the hybrid model against data in the selected .BVA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9583d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction errors for good and bad training data separately\n",
    "errors_good = calc_reconstruction_errors(Float32.(min_max_normalize_matrix(training_data_good)), refined_model)\n",
    "errors_bad = calc_reconstruction_errors(Float32.(min_max_normalize_matrix(training_data_bad)), refined_model)\n",
    "\n",
    "# Normalize new data\n",
    "X_data_32 = Float32.(X_data)\n",
    "mean_train_selected = mean(training_data_good, dims=1)[:, 1:size(X_data)[2]]\n",
    "std_train_selected = std(training_data_good, dims=1)[:, 1:size(X_data)[2]]\n",
    "\n",
    "X_new_normalized = min_max_normalize_matrix(X_data_32)\n",
    "# Make predictions using the trained model\n",
    "\n",
    "predicted_X_data = refined_model(X_new_normalized)\n",
    "reconstruction_error = sum((X_new_normalized .- predicted_X_data) .^ 2, dims=1)\n",
    "reconstruction_error_vector = vec(reconstruction_error)    # convert reconstruction_error matrix to vector\n",
    "#==\n",
    "Note: The reconstruction error is the difference between the original data and the reconstructed data. \n",
    "      This error reflects how well the model can replicate the original data. \n",
    "      A low reconstruction error suggests the model has captured the structure of the data well, \n",
    "          while a high reconstruction error suggests that the data is unusual or anomalous.\n",
    "==#\n",
    "\n",
    "# Normalize reconstruction errors to match scaling with good and bad thresholds\n",
    "normalized_reconstruction_error = min_max_normalize_matrix(reconstruction_error_vector)\n",
    "\n",
    "# Weighting errors for bad data\n",
    "bad_weight_factor = 2.0  # Apply more weight to bad data\n",
    "weighted_errors_bad = errors_bad .* bad_weight_factor\n",
    "\n",
    "# Set separate thresholds\n",
    "good_threshold = quantile(errors_good, 0.95)  \n",
    "bad_threshold = quantile(weighted_errors_bad, 0.995)\n",
    "\n",
    "##threshold = mean(normalized_reconstruction_error) + 3 * std(normalized_reconstruction_error)\n",
    "threshold = quantile(normalized_reconstruction_error, 0.995)\n",
    "\n",
    "println(\"Threshold: \", threshold)\n",
    "println(\"Good data threshold: \", good_threshold)\n",
    "println(\"Bad data threshold: \", bad_threshold)\n",
    "\n",
    "inverted_reconstruction_error = 1.0 .- normalized_reconstruction_error\n",
    "\n",
    "# Calculate adaptive thresholds based on median and standard deviation\n",
    "inverted_median = median(inverted_reconstruction_error)\n",
    "inverted_std = std(inverted_reconstruction_error)\n",
    "\n",
    "# Adaptive uncertain and bad thresholds\n",
    "uncertain_threshold = inverted_median + inverted_std\n",
    "bad_threshold = inverted_median + 1.5 * inverted_std\n",
    "\n",
    "# Identify outliers and uncertain points based on adaptive thresholds\n",
    "outliers = findall(inverted_reconstruction_error .> bad_threshold)\n",
    "uncertain_indices = findall(x -> uncertain_threshold < x <= bad_threshold, inverted_reconstruction_error)\n",
    "\n",
    "# Convert indices to dates if necessary\n",
    "outlier_dates = X_date[outliers]\n",
    "uncertain_dates = X_date[uncertain_indices]\n",
    "\n",
    "# Output results\n",
    "if !isempty(outlier_dates)\n",
    "    println(\"Outliers detected at the following dates:\")\n",
    "    for date in outlier_dates\n",
    "        println(\"    \", Dates.format(date, \"yyyy-mm-dd HH:MM\"))\n",
    "    end\n",
    "else\n",
    "    println(\"No outliers detected.\")\n",
    "end\n",
    "\n",
    "if !isempty(uncertain_dates)\n",
    "    println(\"Uncertain data points detected at the following dates:\")\n",
    "    for date in uncertain_dates\n",
    "        println(\"    \", Dates.format(date, \"yyyy-mm-dd HH:MM\"))\n",
    "    end\n",
    "else\n",
    "    println(\"No uncertain data points detected.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b882316-6daa-41ea-8da9-da10ed3f0af9",
   "metadata": {},
   "source": [
    "### Plot reconstruction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e00a70-38c5-424c-92ea-6cdd20d5fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots: vline!, hspan!, xlims, ylims\n",
    "using Dates: Day, Month, Year, Hour, Minute\n",
    "start_date = DateTime(Year(X_date[1]), Month(X_date[1]), Day(X_date[1])) + Day(1)  # next day's 00:00\n",
    "tick_interval = Hour(2)  # or Hour(6), Day(1), etc., depending on desired interval\n",
    "\n",
    "# Generate tick positions and labels\n",
    "ticks = collect(start_date:tick_interval:X_date[end])\n",
    "tick_labels = Dates.format.(ticks, \"dd HH:MM\")  # Adjust format to show days and hours\n",
    "\n",
    "title=\"Hybrid Model reconstruction errors - \"*split(infil,\"\\\\\")[end]\n",
    "\n",
    "# Set plotting limits of Y-axis\n",
    "y_max = 1.05\n",
    "\n",
    "plot( size=(1200,600), dpi=100, title=title, xlims=(start_time,end_time), ylims=(0,y_max), \n",
    "    xticks=(ticks, tick_labels), xrotation=90, xtickfont=font(7), \n",
    "    yticks = false, ylabel=(\"Reconstruction Error\"),\n",
    "    framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "    legend=:bottomleft, leftmargin=8Plots.mm, bottommargin=5Plots.mm,\n",
    "    grid=true, gridlinewidth=0.125, gridstyle=:dot, gridcolor=:grey, gridalpha=0.5)\n",
    "\n",
    "plot!(X_date, inverted_reconstruction_error, lw=:2, lc=:grey, label=\"\")\n",
    "\n",
    "# Plot bands of error type\n",
    "hspan!([bad_threshold, y_max], fillcolor=:red, fillalpha=:0.125, label=\"Outlier area\")\n",
    "hspan!([uncertain_threshold, bad_threshold], fillcolor=:lightgrey, fillalpha=:0.25, label=\"Uncertain area\")\n",
    "hspan!([0, uncertain_threshold], fillcolor=:green, fillalpha=:0.125, label=\"Valid area\")\n",
    "\n",
    "# Plot location of outliers on X-axis\n",
    "vline!(outlier_dates, ls=:dash, lc=:red, lw=:0.5, label=\"Detected Outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86832c4",
   "metadata": {},
   "source": [
    "### Plot records with suspect data (as identified by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69caf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics: quantile\n",
    "using Plots: hspan!\n",
    "\n",
    "\n",
    "function do_plots(ii, start_time, heave)\n",
    "####################################\n",
    "    \n",
    "    end_time = start_time + Minute(30)\n",
    "    xvals = start_time + Microsecond.((0:REC_LENGTH-1) / SAMPLE_FREQUENCY * 1000000)\n",
    "\n",
    "    Q1 = quantile(heave, 0.25)\n",
    "    Q3 = quantile(heave, 0.75)\n",
    "\n",
    "    multiplier = 1.5\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "\n",
    "    # Plot initialization\n",
    "    p1 = plot(size=(2000, 400), dpi=100, framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "        legend=:topright, xtickfont=font(8), ytickfont=font(8), bottommargin=5Plots.mm, \n",
    "        grid=true, gridlinewidth=0.125, gridstyle=:dot, gridcolor=:grey, gridalpha=0.5)\n",
    "    \n",
    "    p1 = hspan!([lower_bound, upper_bound], fillcolor=:lightblue, fillalpha=:0.125, label=\"IQR limits\")\n",
    "    \n",
    "    tm_tick = range(start_time, end_time, step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick, \"MM\")\n",
    "    \n",
    "    # Calculate dynamic confidence interval\n",
    "    confidence_interval = 3.29 # threshold at the 99.9th percentile level\n",
    "\n",
    "    # Identify z_scores using modified z-score\n",
    "    z_score_indices, mod_z_scores = modified_z_score(heave, confidence_interval)\n",
    "    if !isempty(z_score_indices)\n",
    "        scatter!(p1, xvals[z_score_indices], heave[z_score_indices], \n",
    "            markersize=4, markerstrokecolor=:red, markerstrokewidth=1, \n",
    "            markercolor=:white, markershape=:circle, label=\"Modified Z-score beyond 99.9% confidence limits\")\n",
    "    end\n",
    "\n",
    "    # Plot confidence limits\n",
    "    confidence_limits = calc_confidence_limits(heave, confidence_interval)\n",
    "    hline!(p1, [confidence_limits[1], confidence_limits[2]], color=:red, lw=0.5, linestyle=:dash, label=\"99.9% confidence limits\")\n",
    "\n",
    "    # Plot heave data\n",
    "    plot!(p1, xvals, heave, xlims=(xvals[1], xvals[end]), lw=0.5, lc=:blue, alpha=0.5, \n",
    "        xticks=(tm_tick, ticks), label=\"\")\n",
    "\n",
    "    # Annotate plot with the number of outliers and confidence interval\n",
    "    num_outliers = length(z_score_indices)\n",
    "    suspect_string = string(\"  \", string(ii),\" \",Dates.format(start_time, \"yyyy-mm-dd HH:MM\"), \" - \", num_outliers, \" Possible outliers\") # using Confidence Interval of \", \n",
    "##        @sprintf(\"%.2f\", confidence_interval))\n",
    "    annotate!(p1, xvals[1], maximum(heave) * 0.9, text(suspect_string, :left, 10, :blue))\n",
    "\n",
    "    display(p1)\n",
    "    \n",
    "end    # do_plots()  \n",
    "\n",
    "\n",
    "for ii ∈ outliers\n",
    "\n",
    "    # Initialize variables\n",
    "    start_time = X_date[ii]\n",
    "    global heave = X_data[:, ii]\n",
    "    \n",
    "    do_plots(ii, start_time, heave)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the y-axis limits\n",
    "y_min, y_max = ylims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350bbcd-af87-4cce-b979-26d95b8137a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(inverted_reconstruction_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269085d-b952-4d4b-b5ae-baec487de42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
