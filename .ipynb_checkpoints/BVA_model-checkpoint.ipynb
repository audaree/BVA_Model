{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206c66f8-3910-4c47-b4b3-39b1e6c00773",
   "metadata": {},
   "source": [
    "# Detecting Outliers in Water Surface Elevation Data\n",
    "\n",
    "This process involves using a trained autoencoder model to identify possible outliers in a matrix of water surface elevations (X_data). \n",
    "\n",
    "These outliers may represent unusual wave patterns or anomalies in the data.\n",
    "\n",
    "## Prerequisites:\n",
    "Flux Model: A trained autoencoder model that has been trained on normal water surface elevation data.\n",
    "\n",
    "## Data:\n",
    "X_data: A matrix of water surface elevation values (size: 4608 × number of records), where each column corresponds to a 30-minute time series of surface elevations.\n",
    "\n",
    "X_date: A vector of DateTime values, where each entry corresponds to the starting time of the respective column in X_data.\n",
    "\n",
    "## Steps to Detect Outliers:\n",
    "\n",
    "### 1. Load the Required Libraries and Data\n",
    "Before running the analysis, make sure that you have the necessary libraries and data in place. You will need:\n",
    "\n",
    "    Flux: For the autoencoder model\n",
    "    \n",
    "    Your trained model: It should already be trained on normal data.\n",
    "\n",
    "### 2. Define the Data and Model\n",
    "Make sure your data (X_data and X_date) is loaded and accessible.\n",
    "Ensure your trained model is available.\n",
    "\n",
    "### 3. Run the Model to Predict Reconstructed Data\n",
    "The model will predict reconstructed data from the input X_data. \n",
    "The reconstruction error will be used to determine whether a specific record is an outlier.\n",
    "\n",
    "### 4. Calculate Reconstruction Error\n",
    "The reconstruction error is the difference between the original and reconstructed data. \n",
    "Higher errors indicate that the original data is different from the learned pattern and may be an outlier.\n",
    "\n",
    "### 5. Determine Possible Outliers\n",
    "Now, determine the columns with the highest reconstruction errors, which could be the outliers. \n",
    "For this, you can use a threshold based on the error distribution.\n",
    "\n",
    "### 6. Map Outliers to Date/Time\n",
    "Using the outliers indices, map them to their corresponding time in X_date to know which time periods have possible outliers.\n",
    "\n",
    "### 7. Display the Outliers\n",
    "You can display the dates and times of the detected outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639b23a-7db1-467f-805d-e7334ee0baf6",
   "metadata": {},
   "source": [
    "### Select and read contents of .BVA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc3ff0-a7e2-476d-a3cc-04a731309ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames: DataFrame, ncol, nrow\n",
    "using Dates: Dates, DateTime, Time, unix2datetime, Hour, Minute, Microsecond\n",
    "using NativeFileDialog: pick_file\n",
    "using Statistics: median, mean, std\n",
    "using Plots: Plots, plot, plot!, annotate!, hline!, @layout, text, plotly, font, scatter!\n",
    "using Printf: @sprintf\n",
    "\n",
    "#using PlotlyKaleido\n",
    "\n",
    "#plotly()\n",
    "#PlotlyKaleido.start(timeout=30)\n",
    "\n",
    "\n",
    "function get_matches(Data, f23_df)\n",
    "##################################\n",
    "    \n",
    "    # Create a dictionary to store indices of hex strings in Data\n",
    "    index_dict = Dict{String, Vector{Int}}()\n",
    "    \n",
    "    # Populate the dictionary\n",
    "    for (i, hex_str) in enumerate(Data)\n",
    "        if haskey(index_dict, hex_str)\n",
    "            push!(index_dict[hex_str], i)\n",
    "        else\n",
    "            index_dict[hex_str] = [i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Initialize a vector to store indices\n",
    "    matching_indices = []\n",
    "    \n",
    "    # Iterate through each hex string in f23_df and lookup in the dictionary\n",
    "    for hex_str in f23_df.Match_vector\n",
    "        if haskey(index_dict, hex_str)\n",
    "            push!(matching_indices, index_dict[hex_str][1])\n",
    "        else\n",
    "            push!(matching_indices, nothing)  # If no match found, store an empty vector\n",
    "        end\n",
    "    end\n",
    "\n",
    "    f23_df[!,\"Data_vector\"] = matching_indices\n",
    "\n",
    "    return(f23_df)\n",
    "\n",
    "end    # get_matches()\n",
    "\n",
    "# function to calculate selected parameters from Spectrum synchronisation message (0xF23)\n",
    "function process_f23(f23_vals)\n",
    "#######################################\n",
    "    \n",
    "    # refer to DWTP (Ver. 16 January2019) Section 4.3 pp.43-44\n",
    "\n",
    "    # get Timestamp in UTC\n",
    "    timestamp = unix2datetime(parse(Int, bitstring(f23_vals[3]) * bitstring(f23_vals[4]) * bitstring(f23_vals[5]) * bitstring(f23_vals[6]); base=2))\n",
    "    \n",
    "    # convert time to Australian Eastern Standard Time\n",
    "    timestamp = timestamp + Hour(0)  # Adjust this for the correct time zone\n",
    "\n",
    "    # get Data Stamp\n",
    "    data_stamp = parse(Int, bitstring(f23_vals[7]) * bitstring(f23_vals[8]); base=2)\n",
    "\n",
    "    # get Segments Used\n",
    "    segments_used = parse(Int, bitstring(f23_vals[9]) * bitstring(f23_vals[10]) * bitstring(f23_vals[11]); base=2)\n",
    "\n",
    "    # get Sample Number\n",
    "    sample_number = parse(Int, bitstring(f23_vals[12]) * bitstring(f23_vals[13]); base=2)\n",
    "\n",
    "    # Create Match Vector\n",
    "    match_vector = lpad(string(f23_vals[14], base=16), 2, \"0\")\n",
    "    for i in 15:22\n",
    "        match_vector = match_vector * lpad(string(f23_vals[i], base=16), 2, \"0\")\n",
    "    end\n",
    "    \n",
    "    return(timestamp, segments_used, match_vector, sample_number)\n",
    "    \n",
    "end    #  process_f23()\n",
    "\n",
    "\n",
    "# convert binary data into F23_df and Hex array\n",
    "function get_hex_array(infil)\n",
    "#############################\n",
    "    \n",
    "    # Read binary data from the input file\n",
    "    println(\"Reading BINARY data from \", infil)\n",
    "    flush(stdout)\n",
    "    data = reinterpret(UInt8, read(infil))\n",
    "    \n",
    "    # Turn the data vector into a matrix of 12 values matching hexadecimal bytes\n",
    "    cols = 12\n",
    "    rows = Int(length(data) / cols)\n",
    "    mat = reshape(view(data, :), cols, :)\n",
    "    \n",
    "    # Calculate the Heave, North, and West displacements\n",
    "    hex_matrix = string.(mat'[:,1:9], base=16, pad=2)\n",
    "    Data = [join(row) for row in eachrow(hex_matrix)]\n",
    "    \n",
    "    println(\"All file data read!\")\n",
    "    \n",
    "    # Interleave the last 3 matrix columns (10, 11, 12) to form the packet vector\n",
    "    packet = collect(Iterators.flatten(zip(mat[10,:], mat[11,:], mat[12,:])))\n",
    "    \n",
    "    # Find all occurrences of 0x7e in the packet vector\n",
    "    aa = findall(x -> x == 0x7e, vec(packet))\n",
    "    \n",
    "    # Create DataFrame to hold the processed data\n",
    "    f23_df = DataFrame(Date = DateTime[], Segments = Int[], Match_vector = String[], Sample_number = Int[])\n",
    "    \n",
    "    # Decode the packet data into messages\n",
    "    max_val = length(aa) - 1\n",
    "    \n",
    "    for i in 1:max_val\n",
    "        first = aa[i] + 1\n",
    "        last = aa[i + 1]\n",
    "        \n",
    "        if (last - first > 1)\n",
    "            decoded = packet[first:last-1]\n",
    "            \n",
    "            # Handle the 0x7d escape sequences (XOR with 0x20)\n",
    "            bb = findall(x -> x == 0x7d, decoded)\n",
    "            for ii in bb\n",
    "                decoded[ii + 1] = decoded[ii + 1] ⊻ 0x20\n",
    "            end\n",
    "            deleteat!(decoded, bb)\n",
    "            \n",
    "            # If the message is F23 (0x23)\n",
    "            if decoded[2] == 0x23\n",
    "                timestamp, segments_used, match_vector, sample_number = process_f23(decoded)\n",
    "                push!(f23_df, [timestamp, segments_used, match_vector, sample_number])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Remove duplicates from f23_df\n",
    "    f23_df = unique(f23_df);\n",
    "\n",
    "    return(f23_df, Data)\n",
    "    \n",
    "    end    # get_hex_array()\n",
    "\n",
    "\n",
    "function get_start_end_dates(f23_df,found_list)\n",
    "###############################################\n",
    "    \n",
    "    start_date = f23_df[found_list[1],:].Date - Minute(30) # <------- NOTE subtracted 30min from start_date to match Waves4 results\n",
    "    segments = f23_df[found_list[1],:].Segments\n",
    "#   match_vector = f23_df[found_list[1],:].Match_vector\n",
    "    sample_nos = f23_df[found_list[1],:].Sample_number\n",
    "    data_vector = f23_df[found_list[1],:].Data_vector\n",
    "    start_val = data_vector - Int(sample_nos/2) + 1\n",
    "    end_val = data_vector\n",
    "    \n",
    "    return(start_date,start_val, end_val)\n",
    "    \n",
    "end    #(get_start_end_dates)\n",
    "    \n",
    "  \n",
    "function get_displacement(Data, start_val, end_val)\n",
    "################################################\n",
    "# Decode the real time data to displacements - See DWTP (16 Jan 2019) 2.1.1 p. 19    \n",
    "    \n",
    "    arry = collect(Iterators.flatten(zip(SubString.(Data, start_val, end_val),SubString.(Data, start_val+9, end_val+9))));\n",
    "    displacements = [parse(Int, SubString.(i, 1, 1), base=16)*16^2 + parse(Int, SubString.(i, 2, 2), base=16)*16^1 + parse(Int, SubString.(i, 3, 3), base=16)*16^0 for i in arry]    \n",
    "    \n",
    "    displacements[findall(>=(2048), displacements)] = displacements[findall(>=(2048), displacements)] .- 4096;\n",
    "    displacements = 0.457*sinh.(displacements/457)    # see DWTP p.19 (16)\n",
    "   \n",
    "    return(displacements)\n",
    "    \n",
    "end    # get_displacement()\n",
    "\n",
    "\n",
    "function get_hnw(Data,start_val,end_val)\n",
    "######################################## \n",
    "    \n",
    "    # get WSEs for desired 30-minute record\n",
    "    heave = get_displacement(Data[start_val:end_val,:], 1, 3);              \n",
    "    north = get_displacement(Data[start_val:end_val,:], 4, 6);\n",
    "    west = get_displacement(Data[start_val:end_val,:], 7, 9);\n",
    "    \n",
    "    # Check for missing or extra points in data\n",
    "    for wse in [heave, north, west]\n",
    "        \n",
    "        wse_length = length(wse)\n",
    "        \n",
    "        if wse_length > REC_LENGTH\n",
    "\n",
    "            # truncate if too long\n",
    "            wse = wse[1:REC_LENGTH]\n",
    "            \n",
    "        else\n",
    "\n",
    "            # zero pad if too short (leave it unchanged if right length)\n",
    "            append!(wse,zeros(REC_LENGTH-wse_length))\n",
    "            \n",
    "        end      \n",
    "\n",
    "    end\n",
    "    \n",
    "    return (heave, north, west)\n",
    "    \n",
    "end    # get_hnw()\n",
    "\n",
    "\n",
    "# Function to calculate confidence limits\n",
    "function calc_confidence_limits(data, confidence_interval)\n",
    "##########################################################\n",
    "    \n",
    "    mean_val = mean(data)\n",
    "    std_dev = std(data)\n",
    "    upper_limit = mean_val + confidence_interval * std_dev\n",
    "    lower_limit = mean_val - confidence_interval * std_dev\n",
    "    \n",
    "    return (lower_limit, upper_limit)\n",
    "    \n",
    "end    # calc_confidence_limits()\n",
    "\n",
    "\n",
    "# Function to compute modified z-scores and find outliers\n",
    "function modified_z_score(data, threshold)\n",
    "##########################################\n",
    "    \n",
    "    med = median(data)\n",
    "    mad = median(abs.(data .- med))\n",
    "    mod_z_scores = 0.6745 * (data .- med) ./ mad\n",
    "    outlier_indices = findall(x -> abs(x) > threshold, mod_z_scores)\n",
    "    \n",
    "    return(outlier_indices, mod_z_scores)\n",
    "    \n",
    "end    # modified_z_score()\n",
    "\n",
    "\n",
    "# Function for dynamic threshold based on mean wave height\n",
    "function dynamic_z_score_thresholdXXX(heave, base_threshold=3.0, k=0.5)\n",
    "####################################################################\n",
    "#==\n",
    "    mean_wave_height = mean(heave)\n",
    "    std_wave_height = std(heave)\n",
    "    dynamic_threshold = base_threshold * (1 + k * (mean_wave_height / std_wave_height))\n",
    "==#\n",
    "    max_threshold = 3.29\n",
    "    \n",
    "    heave_range = maximum(heave) - minimum(heave)\n",
    "    std_wave_height = std(heave)\n",
    "\n",
    "    # Scale the threshold based on the range of heave values\n",
    "    # Adjust the scaling factor (e.g., 0.1) as needed to fit your data\n",
    "    scaling_factor = 0.1 * (heave_range / std_wave_height)\n",
    "\n",
    "    # Calculate the dynamic threshold\n",
    "    dynamic_threshold = base_threshold + scaling_factor\n",
    "\n",
    "    # Clamp the threshold between the defined limits\n",
    "    dynamic_threshold = clamp(dynamic_threshold, base_threshold, max_threshold)\n",
    "    \n",
    "    return(dynamic_threshold)\n",
    "    \n",
    "end    # dynamic_z_score_threshold()\n",
    "\n",
    "\n",
    "function pad_or_truncate(record, target_length=REC_LENGTH)\n",
    "####################################################\n",
    "\n",
    "    length(record) < target_length ? vcat(record, zeros(Float32, target_length - length(record))) :\n",
    "                                     record[1:target_length]\n",
    "\n",
    "end    # pad_or_truncate()\n",
    "\n",
    "\n",
    "function get_heave(Data, f23_df)\n",
    "################################\n",
    "    \n",
    "    heave_array = []\n",
    "    X_date = []\n",
    "\n",
    "    println(\"Calculating Heave values now!\")\n",
    "    \n",
    "    for idx in 1:nrow(f23_df)\n",
    "\n",
    "        if !isnothing(f23_df.Data_vector[idx])\n",
    "    \n",
    "            start_date, start_val, end_val = get_start_end_dates(f23_df,idx)\n",
    "            if start_val > 0\n",
    "                print(\".\")\n",
    "                heave, north, west = get_hnw(Data,start_val,end_val)\n",
    "\n",
    "                # ensure we have REC_LENGTH data points\n",
    "                push!(heave_array,pad_or_truncate(heave, REC_LENGTH))\n",
    "                push!(X_date,start_date)\n",
    "            end\n",
    "\n",
    "        end\n",
    "    \n",
    "    end\n",
    "\n",
    "    return(hcat(heave_array...), X_date)\n",
    "\n",
    "end    # get_heave()\n",
    "\n",
    "\n",
    "# Need to check first row of the f23_df in case 23:00 is stored there\n",
    "function f23_first_row_check(f23_df)\n",
    "################################\n",
    "    \n",
    "    # Get the first row of the DataFrame\n",
    "    first_row = first(f23_df)\n",
    "    \n",
    "    # Check if the time of the first row's Date column is 23:00:00\n",
    "    time_of_first_row = Time(first_row.Date)\n",
    "\n",
    "    if time_of_first_row == Time(23, 0, 0)\n",
    "\n",
    "        if ismissing(first_row.Data_vector) || isnothing(first_row.Data_vector) || isnan(first_row.Data_vector)\n",
    "            f23_df = f23_df[2:end, :]  # Drop the first row\n",
    "        end\n",
    "\n",
    "    end\n",
    "    \n",
    "    return(f23_df)\n",
    "    \n",
    "    end    # f23_first_row_check()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "using Sockets\n",
    "\n",
    "#global X_data = Matrix{Float32}(undef, 0, 0)\n",
    "\n",
    "hostname = gethostname()\n",
    "println(\"The name of the computer is: \", hostname)\n",
    "\n",
    "if hostname == \"QUEENSLAND-BASIN\"\n",
    "    \n",
    "    display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "    \n",
    "else\n",
    "    \n",
    "    display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "REC_LENGTH = 4608       # Number of WSE's in a Mk4 30-minute record\n",
    "SAMPLE_FREQUENCY = 2.56 # Mk4 sample frequency in Hertz\n",
    "SAMPLE_LENGTH = 1800    # record length in seconds\n",
    "SAMPLE_RATE = Float64(1/SAMPLE_FREQUENCY) # sample spacing in seconds\n",
    "\n",
    "#########################################################################################################################\n",
    "##    confidence_interval = 2.576  # corresponds to a 99% confidence interval (for a normal distribution)\n",
    "##    confidence_interval = 3.0    # corresponds to a 99.73% confidence interval (for a normal distribution)    \n",
    "##    confidence_interval = 3.29   # corresponds to a 99.9% confidence interval (for a normal distribution)\n",
    "#########################################################################################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "initial_path = \"F:\\\\Card Data\\\\\"\n",
    "infil = pick_file(initial_path)\n",
    "\n",
    "f23_df, Data = get_hex_array(infil)\n",
    "\n",
    "f23_df = get_matches(Data, f23_df)\n",
    "\n",
    "# remove those vectors from F23 df that are not located in the Data vector df\n",
    "f23_df = f23_first_row_check(f23_df)\n",
    "\n",
    "X_data, X_date = get_heave(Data, f23_df);\n",
    "\n",
    "X_data = Float32.(X_data)\n",
    "\n",
    "println(string(length(X_date)),\" records processed.\\n\")\n",
    "println(\"\\nNow plot heave for each record to see suspect data!\")\n",
    "flush(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34574024-756c-424d-aae7-f3a88fd46a9b",
   "metadata": {},
   "source": [
    "### Plot each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc1e9b-777a-472d-8a26-891ee2359971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for dynamic threshold based on wave amplitudes\n",
    "function dynamic_z_score_threshold(heave, base_threshold=3.0, k=0.5)\n",
    "    ####################################################################\n",
    "    \n",
    "    # Calculate the amplitude of heave\n",
    "    amplitude = abs.(heave)\n",
    "    \n",
    "    # Calculate mean and standard deviation of the amplitudes\n",
    "    mean_amplitude = mean(amplitude)\n",
    "    std_amplitude = std(amplitude)\n",
    "    \n",
    "    # Calculate the range of amplitudes\n",
    "    range_amplitude = maximum(amplitude) - minimum(amplitude)\n",
    "    \n",
    "    # Calculate dynamic threshold\n",
    "    dynamic_threshold = base_threshold * (1 + k * (range_amplitude / std_amplitude))\n",
    "    \n",
    "    # Cap the dynamic threshold\n",
    "    max_threshold = 3.29  # or any other maximum threshold you prefer\n",
    "    dynamic_threshold = min(dynamic_threshold, max_threshold)\n",
    "    \n",
    "    return dynamic_threshold\n",
    "end  # dynamic_z_score_threshold()\n",
    "\n",
    "\n",
    "##==\n",
    "# Loop through wave records\n",
    "for ii in 1:length(X_date)\n",
    "    \n",
    "    # Initialize variables\n",
    "    start_time = X_date[ii]\n",
    "    heave = X_data[:, ii]\n",
    "    end_time = start_time + Minute(30)\n",
    "    xvals = start_time + Microsecond.((0:REC_LENGTH-1) / SAMPLE_FREQUENCY * 1000000)\n",
    "\n",
    "    # Plot initialization\n",
    "    p1 = plot(size=(2000, 300), dpi=100, framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "        legend=:topright, xtickfont=font(8), ytickfont=font(8), bottommargin = 10Plots.mm,\n",
    "        grid=true, gridlinewidth=0.125, gridstyle=:dot, gridalpha=1)\n",
    "    \n",
    "    tm_tick = range(start_time, end_time, step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick, \"MM\")\n",
    "    \n",
    "    # Calculate dynamic confidence interval\n",
    "    confidence_interval = dynamic_z_score_threshold(heave)\n",
    "\n",
    "    # Identify z_scores using modified z-score\n",
    "    z_score_indices, mod_z_scores = modified_z_score(heave, confidence_interval)\n",
    "    if !isempty(z_score_indices)\n",
    "        scatter!(p1, xvals[z_score_indices], heave[z_score_indices], \n",
    "            markersize=4, markerstrokecolor=:red, markerstrokewidth=1, \n",
    "            markercolor=:white, markershape=:circle, label=\"\")\n",
    "    end\n",
    "\n",
    "    # Plot confidence limits\n",
    "    confidence_limits = calc_confidence_limits(heave, confidence_interval)\n",
    "    hline!(p1, [confidence_limits[1], confidence_limits[2]], color=:red, lw=0.5, linestyle=:dash, label=\"\")\n",
    "\n",
    "    # Plot heave data\n",
    "    plot!(p1, xvals, heave, xlims=(xvals[1], xvals[end]), lw=0.5, lc=:blue, alpha=0.5, \n",
    "        xticks=(tm_tick, ticks), label=\"\")\n",
    "\n",
    "    # Annotate plot with the number of outliers and confidence interval\n",
    "    num_outliers = length(z_score_indices)\n",
    "    suspect_string = string(\"  \", string(ii),\" \",Dates.format(start_time, \"yyyy-mm-dd HH:MM\"), \" - \", num_outliers, \" Possible outliers\") # using Confidence Interval of \", \n",
    "##        @sprintf(\"%.2f\", confidence_interval))\n",
    "    annotate!(p1, xvals[1], maximum(heave) * 0.9, text(suspect_string, :left, 10, :blue))\n",
    "\n",
    "    display(p1)\n",
    "\n",
    "end\n",
    "#==#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973501b8",
   "metadata": {},
   "source": [
    "### Plot statistics for selected range of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25cd983-15ff-4e24-8e7e-3771c53f6ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using Plots\n",
    "using DSP\n",
    "\n",
    "arry_length = 256\n",
    "\n",
    "for ii in 120:150\n",
    "    \n",
    "    heave = X_data[:, ii]\n",
    "    x_date = Dates.format(X_date[ii], \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "    start_time = X_date[ii]\n",
    "    end_time = start_time + Minute(30)\n",
    "    xvals = start_time + Microsecond.((0:REC_LENGTH-1) / SAMPLE_FREQUENCY * 1000000)\n",
    "\n",
    "    tm_tick = range(start_time, end_time, step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick, \"MM\")\n",
    "\n",
    "    p1 = plot(xvals, heave, size=(2000,300), label=\"\", xlims=(start_time,end_time), framestyle=:box, title=x_date, xticks=(tm_tick, ticks), bottommargin = 10Plots.mm, )\n",
    "\n",
    "    for idx in xvals[collect(1:arry_length:4608)]\n",
    "        p1 = plot!([idx, idx], [minimum(heave), maximum(heave)], label=\"\", color=:red, linestyle=:dash)\n",
    "    end   \n",
    "    #hline!([final_median + (3.29 * final_std)], label=\"Median\")\n",
    "    \n",
    "    suspect_string = string(\"  \", string(ii),\" \",Dates.format(start_time, \"yyyy-mm-dd HH:MM\"))\n",
    "    annotate!(p1, xvals[1], maximum(heave) * 0.9, text(suspect_string, :left, 10, :blue))\n",
    "    \n",
    "    display(p1)\n",
    "\n",
    "    #########################################################################################################################\n",
    "    ##    confidence_interval = 2.576  # corresponds to a 99% confidence interval (for a normal distribution)\n",
    "    ##    confidence_interval = 3.0  # corresponds to a 99.73% confidence interval (for a normal distribution)    \n",
    "    ##    confidence_interval = 3.29  # corresponds to a 99.9% confidence interval (for a normal distribution)\n",
    "    #########################################################################################################################\n",
    "    ##amplitude = abs.(heave)\n",
    "    arry = arraysplit(heave, arry_length, 0)\n",
    "    ##arry = arraysplit(amplitude, arry_length, 0)\n",
    "\n",
    "    p1 = plot(size=(2000,300), xlims=(start_time,end_time), \n",
    "        bottommargin = 10Plots.mm, xticks=(tm_tick, ticks),\n",
    "        fg_legend=:transparent, bg_legend=:transparent,  legend=:topleft, framestyle=:box)\n",
    "\n",
    "    # find the mean of each 256-element array\n",
    "    means = mean.(arry)\n",
    "    medians = median.(arry)\n",
    "    stds = std.(arry)\n",
    "\n",
    "    #find the median value of the means\n",
    "    median_of_means = median(means)\n",
    "\n",
    "    # plot the means of each array\n",
    "    p1 = scatter!(xvals[Int.(collect(0:arry_length:4608-256) .+ arry_length/2)], means, marker=:circle, label=\"Means\")\n",
    "    p1 = scatter!(xvals[Int.(collect(0:arry_length:4608-256) .+ arry_length/2)], medians, marker=:diamond, ms=:5, label=\"Medians\")\n",
    "    p1 = scatter!(xvals[Int.(collect(0:arry_length:4608-256) .+ arry_length/2)], stds, marker=:xcross, markerstrokewidth=5, ms=:5, label=\"Std. Dev's\")\n",
    "\n",
    "    p1 = hline!([(median(stds)+2*std(stds))], ls=:dot, lc=:red, lw=:2, label=\"\")\n",
    "    p1 = hline!([(median(stds)-2*std(stds))], ls=:dot, lc=:red, lw=:2, label=\"\")\n",
    "\n",
    "    for idx in xvals[collect(1:arry_length:4608)]\n",
    "        p1 = plot!([idx, idx], [(median(stds)-2*std(stds)), (median(stds)+2*std(stds))], label=\"\", color=:red, linestyle=:dash)\n",
    "    end \n",
    "    \n",
    "    display(p1)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f98660",
   "metadata": {},
   "source": [
    "### Do spectral plot of selected record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DSP, LinearAlgebra\n",
    "using LombScargle\n",
    "using ToeplitzMatrices\n",
    "using Plots, Printf\n",
    "\n",
    "# Function to calculate AR coefficients using Yule-Walker equations\n",
    "function yule_walker(data, order)\n",
    "#################################\n",
    "# Created in collaboration with OpenAI's ChatGPT.\n",
    "# Uses the Yule-Walker equations to estimate autoregressive (AR) model coefficients.\n",
    "# This function is commonly applied in time series analysis for spectral estimation and noise reduction.\n",
    "    \n",
    "    n = length(data)\n",
    "    \n",
    "    # Autocorrelation estimation\n",
    "    autocorr = [sum(data[1:n-k] .* data[1+k:n]) / n for k in 0:order]\n",
    "    \n",
    "    # Construct the Toeplitz matrix for solving the Yule-Walker equations\n",
    "    R = Toeplitz(autocorr[1:order], autocorr[1:order])\n",
    "    r = autocorr[2:order+1]\n",
    "    \n",
    "    # Solve for AR coefficients\n",
    "    a = R \\ r\n",
    "    \n",
    "    return( vcat(1.0, -a))  # Include 1 for AR model definition\n",
    "    \n",
    "end    # yule_walker()\n",
    "\n",
    "\n",
    "# Function to compute the MEM-based power spectral density\n",
    "function mem_psd(data, order, SAMPLE_RATE, num_points=4096)\n",
    "###########################################################\n",
    "# Created in collaboration with OpenAI's ChatGPT.\n",
    "# Implements the Maximum Entropy Method (MEM) for Power Spectral Density (PSD) estimation.\n",
    "# This method is useful for estimating spectral density, especially for time series with limited data points.\n",
    "    \n",
    "    ar_coeffs = yule_walker(data, order)\n",
    "    \n",
    "    freqs = range(0, stop=SAMPLE_RATE/2, length=num_points)\n",
    "    psd = Float64[]\n",
    "    \n",
    "    for f in freqs\n",
    "        omega = 2 * π * f / SAMPLE_RATE\n",
    "        denom = abs(sum(ar_coeffs .* exp.(-im * omega * (0:order))))\n",
    "        push!(psd, (1 / (denom^2)) * SAMPLE_RATE)  # Normalize by sample rate\n",
    "    end\n",
    "    \n",
    "    return(freqs, psd)\n",
    "    \n",
    "end    # mem_psd()\n",
    "\n",
    "\n",
    "# Band averaging function\n",
    "function band_average(freqs, psd, bin_size)\n",
    "###########################################\n",
    "    \n",
    "    max_freq = maximum(freqs)\n",
    "    bins = 0:bin_size:max_freq\n",
    "    averaged_psd = Float64[]\n",
    "    averaged_freqs = Float64[]\n",
    "\n",
    "    for i in 1:length(bins)-1\n",
    "        mask = (freqs .>= bins[i]) .& (freqs .< bins[i+1])\n",
    "        if any(mask)\n",
    "            avg_psd = mean(psd[mask])\n",
    "            push!(averaged_psd, avg_psd)\n",
    "            push!(averaged_freqs, (bins[i] + bins[i+1]) / 2)  # Center frequency\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return(averaged_freqs, averaged_psd)\n",
    "\n",
    "end    # band_average()\n",
    "\n",
    "\n",
    "# Function to calculate total energy and low-frequency energy\n",
    "function calculate_energy(freqs, psd, low_freq_threshold)\n",
    "#########################################################\n",
    "    \n",
    "    total_energy = sum(psd) * (freqs[2] - freqs[1])  # Area under the entire PSD\n",
    "    low_freq_mask = freqs .<= low_freq_threshold\n",
    "    \n",
    "    low_freq_energy = sum(psd[low_freq_mask]) * (freqs[2] - freqs[1])  # Area under low-frequency PSD\n",
    "    \n",
    "    percentage_low_freq_energy = (low_freq_energy / total_energy) * 100\n",
    "    \n",
    "    return(total_energy, low_freq_energy, percentage_low_freq_energy)\n",
    "    \n",
    "end    # calculate_energy()\n",
    "\n",
    "\n",
    "function normalize_psd(psd)\n",
    "###########################\n",
    "    \n",
    "    return(psd./psd[argmax(psd)])  # Normalize to make the area under the curve equal to 1\n",
    "    \n",
    "end    # normalize_psd()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "SAMPLE_RATE = 2.56\n",
    "NYQUIST = SAMPLE_RATE/2\n",
    "order = 30\n",
    "num_points = 4096\n",
    "low_freq_threshold = 0.06667  # 15s corresponds to this frequency\n",
    "\n",
    "ii = 67\n",
    "arry_length = 256\n",
    "\n",
    "heave = X_data[:, ii]\n",
    "x_date = Dates.format(X_date[ii], \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "\n",
    "##################################\n",
    "# Calculate wave spectra using periodogram()\n",
    "##################################\n",
    "## Compute the periodogram over a full 4096-point segment\n",
    "data_segment = heave\n",
    "psd_result = periodogram(data_segment, fs=SAMPLE_RATE)\n",
    "\n",
    "# Extract frequency and power spectral density values\n",
    "freqs_fft = psd_result.freq\n",
    "psd_fft = psd_result.power\n",
    "\n",
    "bin_size = 0.005\n",
    "averaged_freqs_fft, averaged_psd_fft = band_average(freqs_fft, psd_fft, bin_size)\n",
    "\n",
    "normal_psd_fft = normalize_psd(averaged_psd_fft)\n",
    "\n",
    "##################################\n",
    "# Calculate wave spectra using lombScargle() - see https://juliaastro.org/LombScargle.jl/stable/\n",
    "##################################\n",
    "# Generate timestamps (required as input by the lombscargle() function)\n",
    "timestamps = collect(0:1/2.56:1800)[1:end-1]\n",
    "\n",
    "# Calculate Lomb-Scargle spectrum\n",
    "lombscargle_spectrum = lombscargle(timestamps, heave;\n",
    "                                   minimum_frequency=1e-5,  # Small positive start frequency\n",
    "                                   maximum_frequency=1.28,\n",
    "                                   samples_per_peak=400)\n",
    "\n",
    "# Extract frequency and power values\n",
    "ls_freqpower = freqpower(lombscargle_spectrum)\n",
    "ls_frequencies = ls_freqpower[1]\n",
    "power = ls_freqpower[2]\n",
    "\n",
    "# Remove any remaining Inf values if present (unlikely, but a safeguard)\n",
    "power .= replace(power, Inf => NaN)\n",
    "power .= replace(power, NaN => minimum(skipmissing(power)))\n",
    "\n",
    "# Band-average the Lomb-Scargle spectrum\n",
    "averaged_lombscargle_freqs, averaged_lombscargle_fft = band_average(ls_frequencies, power, bin_size)\n",
    "\n",
    "# Normalize the lombscargle fft\n",
    "normal_lombscargle_fft = normalize_psd(averaged_lombscargle_fft)\n",
    "\n",
    "##################################\n",
    "# Calculate wave spectra using MEM\n",
    "##################################\n",
    "\n",
    "# Compute PSD using MEM\n",
    "freqs_mem, psd_mem = mem_psd(heave, order, SAMPLE_RATE, num_points)\n",
    "\n",
    "# Perform band averaging on MEM results\n",
    "averaged_freqs_mem, averaged_psd_mem = band_average(freqs_mem, psd_mem, bin_size)\n",
    "\n",
    "normal_psd_mem = normalize_psd(averaged_psd_mem)\n",
    "\n",
    "# Calculate energies\n",
    "total_energy, low_freq_energy, percentage_low_freq_energy = calculate_energy(averaged_freqs_fft, averaged_psd_fft, low_freq_threshold)\n",
    "\n",
    "# Output results\n",
    "println(\"Total Energy: $total_energy\")\n",
    "println(\"Low-Frequency Energy: $low_freq_energy\")\n",
    "\n",
    "@printf(\"Percentage of Low-Frequency Energy: %4.2f%%\",percentage_low_freq_energy)\n",
    "\n",
    "# Plot initialization\n",
    "p1 = plot(size=(1200, 800), dpi=100, framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "    xlabel=\"Frequency (Hz)\", xlims=(0,0.6), ylims=(0,Inf),\n",
    "    ylabel=\"Normalized Spectral Density\", title=x_date,\n",
    "    legend=:topright, xtickfont=font(8), ytickfont=font(8),\n",
    "    leftmargin = 20Plots.mm, bottommargin = 20Plots.mm,\n",
    "    grid=true, gridlinewidth=0.125, gridstyle=:dot, gridalpha=1)\n",
    "\n",
    "p1 = plot!(averaged_lombscargle_freqs, normal_lombscargle_fft, lw=:6, lc=:yellow, alpha=:0.5, label=\"Lomb–Scargle FFT spectra\")\n",
    "\n",
    "p1 = plot!(averaged_freqs_fft, normal_psd_fft, lw=:2, lc=:blue, alpha=:0.5, label=\"Periodogram FFT spectra\")\n",
    "\n",
    "p1 = plot!(averaged_freqs_mem, normal_psd_mem, lw=:2, ls=:dot, lc=:pink, alpha=:0.75, label=\"MEM spectra\")\n",
    "\n",
    "p1 = vline!([low_freq_threshold], lw=:2, lc=:red, ls=:dash, label=\"Low frequency cut-off - 15s\\n\")  \n",
    "\n",
    "# Fill the area under the curve from 0 Hz to low frequency cutoff\n",
    "fillrange = zeros(length(averaged_freqs_fft))  # Base level for filling (y=0)\n",
    "fill_mask = averaged_freqs_fft .<= low_freq_threshold  # Mask for frequencies below cutoff\n",
    "\n",
    "# Fill the area under the PSD curve for the specified frequency range\n",
    "p1 = plot!(averaged_freqs_fft[fill_mask], normal_psd_fft[fill_mask], fillrange=fillrange[fill_mask], \n",
    "      fillalpha=0.5, color=:red, label=\"Low frequency energy: \"*  @sprintf(\"%.2f%%\", percentage_low_freq_energy))\n",
    "\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d29eb7",
   "metadata": {},
   "source": [
    "### Plot Percentage of low-freq energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DSP, LinearAlgebra\n",
    "using ToeplitzMatrices\n",
    "using Plots, Printf\n",
    "\n",
    "# Function to calculate AR coefficients using Yule-Walker equations\n",
    "function yule_walker(data, order)\n",
    "#################################\n",
    "    \n",
    "    n = length(data)\n",
    "    \n",
    "    # Autocorrelation estimation\n",
    "    autocorr = [sum(data[1:n-k] .* data[1+k:n]) / n for k in 0:order]\n",
    "    \n",
    "    # Construct the Toeplitz matrix for solving the Yule-Walker equations\n",
    "    R = Toeplitz(autocorr[1:order], autocorr[1:order])\n",
    "    r = autocorr[2:order+1]\n",
    "    \n",
    "    # Solve for AR coefficients\n",
    "    a = R \\ r\n",
    "    \n",
    "    return( vcat(1.0, -a))  # Include 1 for AR model definition\n",
    "    \n",
    "end    # yule_walker()\n",
    "\n",
    "\n",
    "# Function to compute the MEM-based power spectral density\n",
    "function mem_psd(data, order, SAMPLE_RATE, num_points=4096)\n",
    "###########################################################\n",
    "    \n",
    "    ar_coeffs = yule_walker(data, order)\n",
    "    \n",
    "    freqs = range(0, stop=SAMPLE_RATE/2, length=num_points)\n",
    "    psd = Float64[]\n",
    "    \n",
    "    for f in freqs\n",
    "        omega = 2 * π * f / SAMPLE_RATE\n",
    "        denom = abs(sum(ar_coeffs .* exp.(-im * omega * (0:order))))\n",
    "        push!(psd, (1 / (denom^2)) * SAMPLE_RATE)  # Normalize by sample rate\n",
    "    end\n",
    "    \n",
    "    return(freqs, psd)\n",
    "    \n",
    "end    # mem_psd()\n",
    "\n",
    "\n",
    "# Band averaging function\n",
    "function band_average(freqs, psd, bin_size)\n",
    "###########################################\n",
    "    \n",
    "    max_freq = maximum(freqs)\n",
    "    bins = 0:bin_size:max_freq\n",
    "    averaged_psd = Float64[]\n",
    "    averaged_freqs = Float64[]\n",
    "\n",
    "    for i in 1:length(bins)-1\n",
    "        mask = (freqs .>= bins[i]) .& (freqs .< bins[i+1])\n",
    "        if any(mask)\n",
    "            avg_psd = mean(psd[mask])\n",
    "            push!(averaged_psd, avg_psd)\n",
    "            push!(averaged_freqs, (bins[i] + bins[i+1]) / 2)  # Center frequency\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return(averaged_freqs, averaged_psd)\n",
    "\n",
    "end    # band_average()\n",
    "\n",
    "\n",
    "# Function to calculate total energy and low-frequency energy\n",
    "function calculate_energy(freqs, psd, low_freq_threshold)\n",
    "#########################################################\n",
    "    \n",
    "    total_energy = sum(psd) * (freqs[2] - freqs[1])  # Area under the entire PSD\n",
    "    low_freq_mask = freqs .<= low_freq_threshold\n",
    "    \n",
    "    low_freq_energy = sum(psd[low_freq_mask]) * (freqs[2] - freqs[1])  # Area under low-frequency PSD\n",
    "    \n",
    "    percentage_low_freq_energy = (low_freq_energy / total_energy) * 100\n",
    "    \n",
    "    return(total_energy, low_freq_energy, percentage_low_freq_energy)\n",
    "    \n",
    "end    # calculate_energy()\n",
    "\n",
    "\n",
    "function normalize_psd(psd)\n",
    "###########################\n",
    "    \n",
    "    return(psd./psd[argmax(psd)])  # Normalize to make the area under the curve equal to 1\n",
    "    \n",
    "end    # normalize_psd()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "SAMPLE_RATE = 2.56\n",
    "NYQUIST = SAMPLE_RATE/2\n",
    "order = 30\n",
    "num_points = 4096\n",
    "low_freq_threshold = 0.06667  # 15s corresponds to this frequency\n",
    "\n",
    "##arry_length = 256\n",
    "\n",
    "##heave = X_data[:, ii]\n",
    "##x_date = Dates.format(X_date[ii], \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "percentages = []\n",
    "\n",
    "println(\"Rec. No.        Date       Low Freq. %\")\n",
    "\n",
    "# Loop through each column of X_data\n",
    "for i in 1:size(X_data, 2)\n",
    "\n",
    "    # Extract heave data and corresponding date\n",
    "    heave = X_data[:, i]\n",
    "    x_date = Dates.format(X_date[i], \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "    # Compute the periodogram over a full 4096-point segment\n",
    "    data_segment = heave\n",
    "    psd_result = periodogram(data_segment, fs=SAMPLE_RATE)\n",
    "\n",
    "    # Band-average the power spectral density (PSD) results\n",
    "    averaged_freqs_fft, averaged_psd_fft = band_average(psd_result.freq, psd_result.power, bin_size)\n",
    "\n",
    "    # Normalize PSD for energy calculations\n",
    "    total_energy, low_freq_energy, percentage_low_freq_energy = calculate_energy(averaged_freqs_fft, normalize_psd(averaged_psd_fft), low_freq_threshold)\n",
    "\n",
    "    # Set a flag for high % of low-frequency energy or possible spikes\n",
    "    flag = percentage_low_freq_energy >= 5 ? \"<=== anomaly\" : percentage_low_freq_energy >= 1 ? \"<--- spike\" : \"\"\n",
    "\n",
    "    # Print results\n",
    "    @printf(\"  %3d    %s    %.2f%% %s\\n\", i, x_date, percentage_low_freq_energy, flag)\n",
    "    # Store percentage low-frequency energy\n",
    "    push!(percentages, percentage_low_freq_energy)\n",
    "\n",
    "end\n",
    "\n",
    "tm_tick = range(X_date[1], X_date[end], step=Hour(6))\n",
    "ticks = Dates.format.(tm_tick, \"dd HH:MM\")\n",
    "\n",
    "title = Dates.format(X_date[1], \"yyyy-mm-dd HH:MM\")* \" to \"* Dates.format(X_date[end], \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "\n",
    "plot(X_date,percentages, size=(2000,600), dpi=100, lw=1, lc=:blue, alpha=0.5, title=title,\n",
    "    xlims=(X_date[1], X_date[end]), xticks=(tm_tick, ticks), \n",
    "    ylabel=\"Low-frequency energy (%)\",\n",
    "    legend=:topright, xtickfont=font(8), ytickfont=font(8),\n",
    "    leftmargin = 20Plots.mm, bottommargin = 20Plots.mm,\n",
    "    framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "    grid=true, gridlinewidth=0.125, gridstyle=:dot, gridalpha=1, label=\"\")\n",
    "\n",
    "\n",
    "# Define the threshold line at 5%\n",
    "percentage_threshold = 5\n",
    "\n",
    "hline!([percentage_threshold], lw=:2, lc=:red, ls=:dash, label=\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3cbc18f",
   "metadata": {},
   "source": [
    "using Statistics\n",
    "\n",
    "# Function to process heave data in segments\n",
    "function process_segments(heave, segment_size=512)\n",
    "    # Number of segments\n",
    "    num_segments = div(length(heave), segment_size)\n",
    "    \n",
    "    # Initialize arrays to hold medians\n",
    "    segment_medians = Float64[]\n",
    "    \n",
    "    # Loop through each segment\n",
    "    for i in 1:num_segments\n",
    "        start_index = (i - 1) * segment_size + 1\n",
    "        end_index = i * segment_size\n",
    "        \n",
    "        # Calculate amplitude for the segment\n",
    "        segment_amplitude = abs.(heave[start_index:end_index])\n",
    "        \n",
    "        # Calculate median of the segment\n",
    "        median_amplitude = median(segment_amplitude)\n",
    "        push!(segment_medians, median_amplitude)\n",
    "    end\n",
    "    \n",
    "    # Identify and reject the segment with the greatest median value\n",
    "    max_median_index = argmax(segment_medians)\n",
    "    deleteat!(segment_medians, max_median_index)\n",
    "    \n",
    "    # Calculate final median and standard deviation of the remaining segments\n",
    "    final_median = median(segment_medians)\n",
    "    final_std = std(segment_medians)\n",
    "    \n",
    "    return final_median, final_std\n",
    "end  # process_segments()\n",
    "\n",
    "# Process the segments\n",
    "final_median, final_std = process_segments(heave)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99412f8c",
   "metadata": {},
   "source": [
    "### Select directory containing .BVA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a49f83-effa-4fe5-8d4a-f3c91a436a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames: DataFrame, ncol, nrow\n",
    "using Dates: Dates, DateTime, Time, unix2datetime, Year, Month, Day, Hour, Minute, Microsecond\n",
    "using NativeFileDialog: pick_folder\n",
    "using Statistics: median, mean, std\n",
    "using Plots: Plots, plot, plot!, annotate!, hline!, @layout, text, plotly, font, scatter!\n",
    "using Printf: @sprintf\n",
    "using DSP: periodogram\n",
    "\n",
    "# Compute power spectrum using periodogram\n",
    "function calc_spectra(heave, fs)\n",
    "################################\n",
    "    \n",
    "    ps = periodogram(heave, fs=fs)\n",
    "\n",
    "    # Define the averaging width for 0.005 Hz band spacing\n",
    "    bin_width = Int(round(0.005 / (ps.freq[2] - ps.freq[1])))  # Number of points per band\n",
    "    \n",
    "    # Preallocate arrays for the averaged frequencies and powers\n",
    "    freqs = []\n",
    "    powers = []\n",
    "    \n",
    "    # Start from the first frequency we want to capture\n",
    "    target_freq = 0.005\n",
    "    \n",
    "    # Get the maximum frequency limit\n",
    "    max_freq = maximum(ps.freq)\n",
    "    \n",
    "    # Loop through the power spectrum to calculate the averages\n",
    "    for i in 1:bin_width:length(ps.power) - bin_width\n",
    "        \n",
    "        # Calculate the mean frequency for the current segment\n",
    "        current_freq = mean(ps.freq[i:i+bin_width-1])\n",
    "        \n",
    "        # While the current frequency is less than or equal to the target frequency, capture it\n",
    "        while current_freq >= target_freq && target_freq <= max_freq\n",
    "            push!(freqs, target_freq)\n",
    "            push!(powers, mean(ps.power[i:i+bin_width-1]))\n",
    "            target_freq += 0.005  # Increment the target frequency\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    # Check if the last target_freq needs to be added\n",
    "    if target_freq <= max_freq\n",
    "        push!(freqs, target_freq)\n",
    "        push!(powers, mean(ps.power[end-bin_width+1:end]))  # Average for the last bin\n",
    "    end\n",
    "\n",
    "    return(freqs, powers) \n",
    "\n",
    "end    # calc_spectra()\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 140% !important; }</style>\"))\n",
    "\n",
    "sample_frequency = 2.56\n",
    "\n",
    "# select BVA directory\n",
    "bva_directory = pick_folder()\n",
    "\n",
    "# build list of all bva files in selected directory\n",
    "bva_files = filter(x->occursin(\".BVA\",uppercase(x)), readdir(bva_directory))\n",
    "bva_files = filter(x -> x != \"19700101.BVA\", bva_files)\n",
    "\n",
    "for ii in 1:length(bva_files)\n",
    "    println(ii,\" \",bva_files[ii])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ad382",
   "metadata": {},
   "source": [
    "### Do spectral plot of each record in selected .BVA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51606fd2-eb20-4113-a3be-0f7f6e716b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "p1 = plot(size=(800,600), xlims=(0,0.6), ylims=(0,Inf), framestyle=:box)\n",
    "infil = \"\"\n",
    "\n",
    "for ii in 21 #1:length(bva_files)\n",
    "\n",
    "    infil = bva_directory * \"\\\\\" * bva_files[ii]\n",
    "\n",
    "    f23_df, Data = get_hex_array(infil)\n",
    "\n",
    "    f23_df = get_matches(Data, f23_df)\n",
    "\n",
    "    # remove those vectors from F23 df that are not located in the Data vector df\n",
    "    f23_df = f23_first_row_check(f23_df)\n",
    "    \n",
    "    global X_data, X_date = get_heave(Data, f23_df)\n",
    "\n",
    "    num_cols = size(X_data)[2]\n",
    "\n",
    "    global spectra = Matrix{Float64}(undef, 256, num_cols)\n",
    "\n",
    "    for jj in 1:num_cols\n",
    "\n",
    "        heave = X_data[:,jj]\n",
    "\n",
    "        f2, Pden2 = calc_spectra(heave, sample_frequency)\n",
    "        spectra[:, jj] = Pden2\n",
    "\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "x_fill = 1/15    # set 15s as cut-off limit for low energy part of spectra\n",
    "y_max = maximum(spectra) # maximum spectra value in matrix\n",
    "\n",
    "infil_path = split(infil,\"\\\\\")\n",
    "site_name = infil_path[end-2]\n",
    "infil_name = infil_path[end]\n",
    "\n",
    "title = site_name * \" \" * Dates.format.(X_date[1], \"yyyy-mm-dd HH:MM\") * \" to \" * Dates.format.(X_date[end], \"yyyy-mm-dd HH:MM\")\n",
    "p1 = plot(size=(800,600), xlims=(0,0.6), ylims=(0,Inf), title=title, framestyle=:box)\n",
    "\n",
    "# Fill the area to the left of the vertical line\n",
    "p1 = plot!([0, x_fill, x_fill, 0], [0, 0, y_max, y_max], seriestype=:shape, alpha=0.035, color=:red, label=\"\")\n",
    "\n",
    "f2 = 0.005:0.005:1.28\n",
    "p1 = plot!(f2, spectra, lw=:0.5, lc=:lightgrey, label=\"\")\n",
    "\n",
    "p1 = vline!([x_fill], lw=:1, lc=:red, ls=:dash, label=\"\")\n",
    "\n",
    "try       \n",
    "    plot_file = \".\\\\Plots\\\\\" * site_name * \"_\" * replace(infil_name, \".BVA\" => \"_spectral_plot.png\")\n",
    "\n",
    "    # Output plot file name\n",
    "##    savefig(plot_file)\n",
    "    println(\"\\nPlot file saved as \",plot_file)\n",
    "catch\n",
    "    \"Alert: Plot not saved!\"\n",
    "    flush(stdout)\n",
    "end\n",
    "\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d285210",
   "metadata": {},
   "source": [
    "### Do contour plot of spectra for both frequency (Hz) and wave period (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123509d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using Dates: Dates, Date, DateTime, Time, unix2datetime, Year, Month, Day, Hour, Minute, Microsecond\n",
    "\n",
    "# Define your starting point for midnight after the first X_date entry\n",
    "first_midnight = DateTime(Date(X_date[1]) + Day(1))\n",
    "\n",
    "# display plots to screen\n",
    "tm_tick = range(first_midnight,last(X_date),step=Hour(6))\n",
    "ticks = Dates.format.(tm_tick, \"dd/mm HH00\")\n",
    "\n",
    "title = site_name * \" \" * Dates.format.(X_date[1], \"yyyy-mm-dd HH:MM\") * \" to \" * Dates.format.(X_date[end], \"yyyy-mm-dd HH:MM\")\n",
    "p1 = plot(xlabel=\"Date\", ylabel=\"Frequency (Hz.)\", xtickfontsize=7, \n",
    "    leftmargin = 15Plots.mm, bottommargin = 15Plots.mm,)\n",
    "\n",
    "p1 = contourf!(X_date, f2, spectra, lw=0.25, c=cgrad(:Spectral, rev=true), clims=(0,y_max), levels=10, fill=true, ylims=(0,0.4), xticks=(tm_tick,ticks))\n",
    "\n",
    "# draw grid lines on plot\n",
    "for i in 0:0.1:0.6\n",
    "    p1 = hline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "end\n",
    "\n",
    "for i in tm_tick\n",
    "    p1 = vline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "end\n",
    "\n",
    "p1 = hline!(p1, [1/15], lw=1, c=:yellow, ls=:dash, label=\"\")\n",
    "\n",
    "# Convert frequency (f2) to period (T) and reverse both periods and spectra for ascending order\n",
    "periods = reverse(1.0 ./ f2)       # periods in ascending order\n",
    "spectra_reversed = reverse(spectra, dims=1)  # reverse spectra along y-axis (rows)\n",
    "\n",
    "# get peak spectral value in matrix\n",
    "peak_spectra = 1/f2[argmax(spectra)[1]]\n",
    "\n",
    "# Define y_ticks period labels in seconds\n",
    "y_ticks = [5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 250]\n",
    "\n",
    "y_ticks = y_ticks[1:searchsortedfirst(y_ticks, peak_spectra)+1]\n",
    "\n",
    "# Plot with converted y-axis and reversed spectra\n",
    "p2 = plot(xlabel=\"Date\", ylabel=\"Period (seconds)\", xtickfontsize=7, \n",
    "    leftmargin = 15Plots.mm, bottommargin = 15Plots.mm,)\n",
    "contourf!(p2, X_date, periods, spectra_reversed, lw=0.25, c=cgrad(:Spectral, rev=true), \n",
    "          clims=(0, y_max), levels=10, fill=true, ylims=(0, y_ticks[end]), \n",
    "          xticks=(tm_tick, ticks), yticks=(y_ticks))\n",
    "\n",
    "# Draw grid lines\n",
    "for i in y_ticks\n",
    "    hline!(p2, [i], lw=0.5, c=:white, label=\"\")\n",
    "end\n",
    "\n",
    "for i in tm_tick\n",
    "    vline!(p2, [i], lw=0.5, c=:white, label=\"\")\n",
    "end\n",
    "\n",
    "p2 = hline!(p2, [15], lw=1, c=:red, ls=:dash, label=\"\")\n",
    "\n",
    "p1_p2 = plot(p1, p2, layout=(2,1), size=(1400, 800), framestyle=:box, suptitle = title)\n",
    "\n",
    "try    \n",
    "    infil_path = split(infil,\"\\\\\")\n",
    "    site_name = infil_path[end-2]\n",
    "    infil_name = infil_path[end]\n",
    "    \n",
    "    plot_file = \".\\\\Plots\\\\\" * site_name * \"_\" * replace(infil_name, \".BVA\" => \"_contour_plot.png.png\")\n",
    "\n",
    "    # Output plot file name\n",
    "    savefig(plot_file)\n",
    "    println(\"\\nPlot file saved as \",plot_file)\n",
    "catch\n",
    "    \"Alert: Plot not saved!\"\n",
    "    flush(stdout)\n",
    "end\n",
    "\n",
    "display(p1_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SAMPLE_RATE = 2.56  # Define sample rate\n",
    "bin_size = 0.005\n",
    "low_freq_threshold = 1/15  # Example threshold for low-frequency energy calculation\n",
    "\n",
    "# Pre-allocate percentages array\n",
    "percentages = Float64[]\n",
    "\n",
    "@time begin\n",
    "    \n",
    "    # Loop through each column of X_data\n",
    "\n",
    "    for i in 1:size(X_data, 2)\n",
    "\n",
    "        # Extract heave data and corresponding date\n",
    "        heave = X_data[:, i]\n",
    "        x_date = Dates.format(X_date[i], \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "        # Compute the periodogram over a full 4096-point segment\n",
    "        data_segment = heave[1:4096]\n",
    "        psd_result = periodogram(data_segment, fs=SAMPLE_RATE)\n",
    "\n",
    "        # Band-average the power spectral density (PSD) results\n",
    "        averaged_freqs_fft, averaged_psd_fft = band_average(psd_result.freq, psd_result.power, bin_size)\n",
    "\n",
    "        # Normalize PSD for energy calculations\n",
    "        total_energy, low_freq_energy, percentage_low_freq_energy = calculate_energy(\n",
    "            averaged_freqs_fft, normalize_psd(averaged_psd_fft), low_freq_threshold)\n",
    "\n",
    "        # Set a flag for high % of low-frequency energy or possible spikes\n",
    "        flag = percentage_low_freq_energy >= 5 ? \"<=== anomaly\" : percentage_low_freq_energy >= 1 ? \"<--- spike\" : \"\"\n",
    "\n",
    "        # Print results\n",
    "    #    println(\"$i $x_date - Low-Frequency Energy: $(round(percentage_low_freq_energy, digits=2))% $flag\")\n",
    "        @printf(\"%3d %s - Low-Frequency Energy: %.2f%% %s\\n\", i, x_date, percentage_low_freq_energy, flag)\n",
    "        # Store percentage low-frequency energy\n",
    "        push!(percentages, percentage_low_freq_energy)\n",
    "\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotly()\n",
    "\n",
    "x = 1:length(X_date)\n",
    "y = 0.005:0.005:1.28\n",
    "\n",
    "title = site_name * \" \" * Dates.format.(X_date[1], \"yyyy-mm-dd HH:MM\") * \" to \" * Dates.format.(X_date[end], \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "surface(x, y, spectra, size=(1400,1200), xlabel=\"Date\", ylabel=\"Frequency (Hertz)\", zlabel=\"Spectral Density (m²/Hz)\", c=cgrad(:Spectral, rev=true), title=title, framestyle=:box, colorbar=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff0fef-47e0-45eb-ba84-75fb624a1181",
   "metadata": {},
   "source": [
    "# Model programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3757552-83eb-4359-b7ef-af4683517fad",
   "metadata": {},
   "source": [
    "### Separate data into Good and Bad matricies for training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636c402-9cf7-4749-be6a-b3d070654e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tk\n",
    "\n",
    "\n",
    "# Function to handle a date selection (left mouse click or double-click)\n",
    "function handle_selection()\n",
    "###########################\n",
    "    \n",
    "    selected_item = get_value(lb)  # Get current selection\n",
    "    if selected_item !== nothing  # Check if something is selected\n",
    "        # Add the selected date to the list if not already present\n",
    "        if selected_item[1] ∉ bad_dates\n",
    "            push!(bad_dates, selected_item[1])  # Add to bad_dates if not already there\n",
    "        else\n",
    "            deleteat!(bad_dates, findfirst(x -> x == selected_item[1], bad_dates))  # Deselect if already selected\n",
    "        end\n",
    "        println(\"Current bad_dates: \", selected_item[1])\n",
    "    else\n",
    "        println(\"No date selected!\")\n",
    "    end\n",
    "    \n",
    "end    # handle_selection()\n",
    "\n",
    "# Callback function for the Exit button to process and close\n",
    "function exit_callback()\n",
    "########################\n",
    "    \n",
    "    global new_training_indicies_bad = findall(x -> x ∈ bad_dates, date_string)    # index of bad records in .BVA file\n",
    "    global new_training_indicies_good = findall(x -> x ∉ bad_dates, date_string)\n",
    "\n",
    "    # Populate matrices and vectors based on selections\n",
    "    global new_training_data_good = new_training_data[:, new_training_indicies_good]\n",
    "    global new_training_date_good = new_training_date[new_training_indicies_good]\n",
    "    global new_training_data_bad = new_training_data[:, new_training_indicies_bad]\n",
    "    global new_training_date_bad = new_training_date[new_training_indicies_bad]\n",
    "    global new_training_labels = vcat(fill(:good, size(new_training_data_good, 2)), fill(:bad, size(new_training_data_bad, 2)))\n",
    "\n",
    "    \n",
    "    # Close the window after processing\n",
    "    destroy(w)\n",
    "    \n",
    "end    # exit_callback()\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "###########################################################################################\n",
    "###########################################################################################\n",
    "\n",
    "# use currently selected .BVA file data\n",
    "new_training_data = X_data\n",
    "new_training_date = X_date\n",
    "\n",
    "# build string vector of Dates\n",
    "date_string = Dates.format.(new_training_date, \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "# Initialize list of bad dates\n",
    "global bad_dates = String[]\n",
    "\n",
    "# Set up the Tk window\n",
    "w = Toplevel(\"Select Bad Dates\", 300, 600)\n",
    "tcl(\"pack\", \"propagate\", w, false)\n",
    "f = Frame(w)\n",
    "pack(f, expand=true, fill=\"both\")\n",
    "\n",
    "# Treeview setup to display dates\n",
    "f1 = Frame(f)\n",
    "lb = Treeview(f1, date_string)\n",
    "scrollbars_add(f1, lb)\n",
    "pack(f1, expand=true, fill=\"both\")\n",
    "\n",
    "# Button to finalize selection of bad dates\n",
    "tcl(\"ttk::style\", \"configure\", \"TButton\", foreground=\"blue\", font=\"arial 16 bold\")\n",
    "exit_button = Button(f, \"Exit\")\n",
    "pack(exit_button)\n",
    "\n",
    "# Bind the Exit button to finalize selections and close the window\n",
    "bind(exit_button, \"command\") do path\n",
    "    exit_callback()  # Process the selections when exit button is clicked\n",
    "end\n",
    "\n",
    "# Bind double-click event to the Treeview\n",
    "bind(lb, \"<Double-1>\") do event\n",
    "    handle_selection()  # Handle selection on double-click\n",
    "end\n",
    "\n",
    "println(\"$(length(bad_dates)) bad records selected\")\n",
    "flush(stdout)\n",
    "\n",
    "##new_training_labels = vcat(fill(:good, size(new_training_data_good, 2)), fill(:bad, size(new_training_data_bad, 2)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"$(length(bad_dates)) bad records selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d2928-ec31-4e96-afb2-141e5a42405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_data_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587534d-e79f-4fff-967d-5f0aaed3c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb1828",
   "metadata": {},
   "source": [
    "### Save updated training data for file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Dates\n",
    "\n",
    "##training_data_good = hcat(training_data_good, new_training_data_good)\n",
    "training_data_bad = hcat(training_data_bad, new_training_data_bad)\n",
    "\n",
    "println(\"Good data now \",string(size(training_data_good)[2]),\" records\")\n",
    "println(\"Bad  data now \",string(size(training_data_bad)[2]),\" records\\n\")\n",
    "\n",
    "outfil = \"BVA_training_data_updated_\" * Dates.format(now(), \"yyyy_mm_dd_HHMM\") * \".JLD2\" \n",
    "\n",
    "# Save the updated good and bad training data\n",
    "@save outfil training_data_good training_data_bad\n",
    "\n",
    "println(\"Updated training data saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90408c9d-dded-4875-ac8d-2ea34f412617",
   "metadata": {},
   "source": [
    "### Recover earlier separated data from file (Note: does not include Model data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baef9c-889f-464f-853f-cd7037564376",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "\n",
    "using NativeFileDialog\n",
    "using FilePathsBase\n",
    "\n",
    "# Load the model and optimizer states from the JLD2 file\n",
    "current_path = pwd() * \"\\\\Training_data\"\n",
    "filterlist = \"JLD2\"\n",
    "infil = pick_file(current_path; filterlist)\n",
    "\n",
    "# Load all saved data and labels\n",
    "##@load infil training_data training_date training_data_good training_date_good training_data_bad training_date_bad training_indicies_good training_indicies_bad training_labels # median_train std_train\n",
    "@load infil training_data_good training_data_bad training_data_bad\n",
    "\n",
    "println(\"Data and labels loaded successfully.\")\n",
    "println(\"Good data contains \",string(size(training_data_good)[2]),\" records\")\n",
    "println(\"Bad  data contains \",string(size(training_data_bad)[2]),\" records\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b4bf3-1ffc-4558-9dd2-db461d22e9ef",
   "metadata": {},
   "source": [
    "### Build Model using Good and Bad training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af4bd3-beed-4d43-97b3-890baa363bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "\n",
    "function min_max_normalize_matrix(X)\n",
    "####################################\n",
    "    \n",
    "    min_vals = minimum(X, dims=1)  # Compute min for each column\n",
    "    max_vals = maximum(X, dims=1)  # Compute max for each column\n",
    "    \n",
    "    return((X .- min_vals) ./ (max_vals .- min_vals))\n",
    "    \n",
    "end    # min_max_normalize_matrix()\n",
    "\n",
    "\n",
    "function z_score_normalize_matrix(X)\n",
    "####################################\n",
    "    \n",
    "    mean_vals = mean(X, dims=1)  # Mean for each column\n",
    "    std_vals = std(X, dims=1)    # Standard deviation for each column\n",
    "    \n",
    "    return((X .- mean_vals) ./ std_vals)\n",
    "    \n",
    "end    # z_score_normalize_matrix()\n",
    "\n",
    "\n",
    "function pad_or_truncate(record, target_length=4608)\n",
    "####################################################\n",
    "\n",
    "    length(record) < target_length ? vcat(record, zeros(Float32, target_length - length(record))) :\n",
    "                                     record[1:target_length]\n",
    "\n",
    "end    # pad_or_truncate()\n",
    "\n",
    "\n",
    "function get_heave(Data, f23_df)\n",
    "################################\n",
    "    \n",
    "    heave_array = []\n",
    "    training_date = []\n",
    "    \n",
    "    for idx in 1:nrow(f23_df)\n",
    "\n",
    "        if !isnothing(f23_df.Data_vector[idx])\n",
    "    \n",
    "            start_date, start_val, end_val = get_start_end_dates(f23_df,idx)\n",
    "            if start_val > 0\n",
    "                print(\".\")\n",
    "                heave, north, west = get_hnw(Data,start_val,end_val)\n",
    "\n",
    "                # ensure we have 4608 data points\n",
    "                push!(heave_array,pad_or_truncate(heave, 4608))\n",
    "                push!(training_date,start_date)\n",
    "            end\n",
    "\n",
    "        end\n",
    "    \n",
    "    end\n",
    "\n",
    "    return(hcat(heave_array...), training_date)\n",
    "\n",
    "end    # get_heave()\n",
    "\n",
    "\n",
    "function calc_reconstruction_errors(training_data_float32, model)\n",
    "####################################################\n",
    "    \n",
    "    reconstruction_errors = Float32[]\n",
    "    \n",
    "    for record in eachcol(training_data_float32)  # Each record is now a column with 14 features\n",
    "        reconstructed_record = model(record)  # Pass the record to the autoencoder\n",
    "        error = mean((reconstructed_record .- record).^2)  # Calculate the reconstruction error\n",
    "        push!(reconstruction_errors, error)  # Store the error\n",
    "    end\n",
    "    \n",
    "    return(reconstruction_errors)\n",
    "\n",
    "end    # calc_reconstruction_errors()\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "# Define autoencoder model\n",
    "initial_model = Chain(\n",
    "    Dense(4608, 128, relu),  # Encoder\n",
    "    Dense(128, 64, relu),    # Bottleneck\n",
    "    Dense(64, 128, relu),    # Decoder\n",
    "    Dense(128, 4608)         # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "refined_model = Chain(\n",
    "    Dense(4608, 256, relu),   # Encoder start\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 64, relu),     # bottleneck layer 64 nodes\n",
    "    Dense(64, 128, relu),\n",
    "    Dense(128, 256, relu),    # Decoder end\n",
    "    Dense(256, 4608)          # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "# Define the loss function (e.g., Mean Squared Error for reconstruction)\n",
    "loss(x) = Flux.mse(refined_model(x), x)\n",
    "\n",
    "#==\n",
    "Optimizer: Adam with default parameters (learning rate, etc.)\n",
    "-------------------------------------------------------------\n",
    "In Flux.jl (and most deep learning frameworks), optimizers like Adam come with predefined default values for certain parameters, such as:\n",
    "\n",
    "Learning Rate (α): Controls the step size at each iteration to minimize the loss function. The default is usually set at 0.001.\n",
    "β1 and β2: Decay rates for the moving averages of the gradient and squared gradient, respectively. The default values are typically:\n",
    "    β1 = 0.9\n",
    "    β2 = 0.999\n",
    "Epsilon (ε): A small constant added to prevent division by zero. This is usually set to 1e-8.\n",
    "==#\n",
    "\n",
    "opt = Adam()\n",
    "#opt = AMSGrad(0.001)\n",
    "#opt = Momentum(0.01, 0.9)\n",
    "\n",
    "# Concatenate good and bad data\n",
    "training_data_combined = hcat(training_data_good, training_data_bad)\n",
    "\n",
    "# Normalize the combined data\n",
    "training_data_normalized = min_max_normalize_matrix(training_data_combined)\n",
    "    \n",
    "# Convert WSE data to Float32\n",
    "training_data_float32 = Float32.(training_data_normalized)\n",
    "\n",
    "@time begin\n",
    "    \n",
    "    # Calculate the reconstruction errors (optional)\n",
    "    reconstruction_errors_model = calc_reconstruction_errors(training_data_float32, refined_model)\n",
    "    \n",
    "    # Prepare data for training\n",
    "    data = Iterators.repeated((training_data_float32,), 100)  # data iteration for training\n",
    "    \n",
    "    # Train the model\n",
    "    Flux.train!(loss, Flux.params(refined_model), data, opt)\n",
    "    \n",
    "    println(\"Done!\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56368c63-c7c2-4811-8ff5-a2556e3cf869",
   "metadata": {},
   "source": [
    "### Build Model using mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf8302-2650-4dc6-a7f5-8b1d4e50a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Random, Statistics\n",
    "\n",
    "# Define mini-batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Function to create mini-batches\n",
    "function create_mini_batches(data, batch_size)\n",
    "##############################################\n",
    "    \n",
    "    num_samples = size(data, 2)  # Number of columns in `data` (each column is a record)\n",
    "    shuffle_indices = randperm(num_samples)  # Shuffle indices for randomness\n",
    "    mini_batches = []\n",
    "    for i in 1:batch_size:num_samples\n",
    "        end_idx = min(i + batch_size - 1, num_samples)\n",
    "        push!(mini_batches, data[:, shuffle_indices[i:end_idx]])\n",
    "    end\n",
    "    \n",
    "    return(mini_batches)\n",
    "    \n",
    "end    # create_mini_batches()\n",
    "\n",
    "\n",
    "# Training function with mini-batches\n",
    "function train_model_with_mini_batches(model, data, loss_fn, opt, num_epochs=10, batch_size=64)\n",
    "###############################################################################################\n",
    "    \n",
    "    for epoch in 1:num_epochs\n",
    "        mini_batches = create_mini_batches(data, batch_size)\n",
    "        for mini_batch in mini_batches\n",
    "            Flux.train!(loss_fn, Flux.params(model), [(mini_batch,)], opt)\n",
    "        end\n",
    "        println(\"Completed epoch $epoch\")\n",
    "    end\n",
    "    \n",
    "end    # train_model_with_mini_batches()\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "# Define the refined autoencoder model (unchanged)\n",
    "refined_model = Chain(\n",
    "    Dense(4608, 256, relu),\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 32, relu),\n",
    "    Dense(32, 128, relu),\n",
    "    Dense(128, 256, relu),\n",
    "    Dense(256, 4608)\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "loss(x) = Flux.mse(refined_model(x), x)\n",
    "\n",
    "# Define the optimizer\n",
    "opt = Adam()\n",
    "\n",
    "# Prepare normalized training data (as before)\n",
    "training_data_combined = training_data_good #hcat(training_data_good, training_data_bad)\n",
    "training_data_normalized = min_max_normalize_matrix(training_data_combined)\n",
    "training_data_float32 = Float32.(training_data_normalized)\n",
    "\n",
    "# Train the model using mini-batches\n",
    "num_epochs = 10  # Define the number of epochs for training\n",
    "train_model_with_mini_batches(refined_model, training_data_float32, loss, opt, num_epochs, batch_size)\n",
    "\n",
    "println(\"Training complete with mini-batch processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d64eea-540f-46b7-afde-df0625f3fc5b",
   "metadata": {},
   "source": [
    "### Run the model against data in the selected .BVA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db2614-517a-4c6e-a82b-bbbaf500b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std for each column (record) in the training data (training_data_good)\n",
    "mean_train = mean(training_data_good, dims=1)  # 1×number of records: mean of each record (column) in training_data_good\n",
    "std_train = std(training_data_good, dims=1)    # 1×number of records: std of each record (column) in training_data_good\n",
    "\n",
    "X_data_32 = Float32.(X_data)\n",
    "\n",
    "# Now normalize X_data_32 (4608×191) using the first 191 records of mean_train and std_train\n",
    "# Transpose mean_train and std_train to be broadcasted correctly\n",
    "mean_train_selected = mean_train[:, 1:size(X_data)[2]]  # determine the number of columns in X_data and select this number of values from mean_train\n",
    "std_train_selected = std_train[:, 1:size(X_data)[2]]    \n",
    "\n",
    "# Normalize X_data_32\n",
    "X_new_normalized = Float32.((X_data_32 .- mean_train_selected) ./ std_train_selected)\n",
    "\n",
    "# Step 1: Make predictions using the trained model\n",
    "predicted_X_data = refined_model(X_new_normalized)\n",
    "\n",
    "# Step 2: Calculate reconstruction error (MSE)\n",
    "reconstruction_error = sum((X_new_normalized .- predicted_X_data) .^ 2, dims=1)\n",
    "\n",
    "# Flatten the reconstruction error matrix into a 1D vector\n",
    "reconstruction_error_vector = vec(reconstruction_error)\n",
    "\n",
    "# Step 3: Set a threshold for outlier detection (e.g., 99th percentile)\n",
    "threshold = quantile(reconstruction_error_vector, 0.995)\n",
    "\n",
    "# Step 4: Identify outliers based on the threshold\n",
    "outliers = findall(reconstruction_error .> threshold)\n",
    "global outlier_indices = [idx[2] for idx in outliers]\n",
    "\n",
    "# Step 5: Get the corresponding dates for the outliers\n",
    "outlier_dates = X_date[outlier_indices]\n",
    "\n",
    "if !isempty(outlier_dates)\n",
    "\n",
    "    # Print the outliers\n",
    "    println(\"Outliers detected at the following dates: \")\n",
    "    for ii in outlier_dates\n",
    "        date_string = Dates.format(ii, \"yyyy-mm-dd HH:MM\")\n",
    "        println(\"    \", date_string)\n",
    "    end\n",
    "    \n",
    "else\n",
    "\n",
    "    println(\"No outliers detected\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011364a-cd87-4516-8316-c93d3019f4ae",
   "metadata": {},
   "source": [
    "### Build the Model using a hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225f1bc-b91e-4647-beca-8cc9cd820ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "\n",
    "# (Keep existing functions: min_max_normalize_matrix, z_score_normalize_matrix, pad_or_truncate, get_heave)\n",
    "\n",
    "function min_max_normalize_matrix(X)\n",
    "####################################\n",
    "    \n",
    "    min_vals = minimum(X, dims=1)  # Compute min for each column\n",
    "    max_vals = maximum(X, dims=1)  # Compute max for each column\n",
    "    \n",
    "    return((X .- min_vals) ./ (max_vals .- min_vals))\n",
    "    \n",
    "end    # min_max_normalize_matrix()\n",
    "\n",
    "\n",
    "function calc_reconstruction_errors(data_matrix, model)\n",
    "#######################################################\n",
    "    \n",
    "    reconstruction_errors = Float32[]\n",
    "    for record in eachcol(data_matrix)\n",
    "        reconstructed_record = model(record)\n",
    "        error = mean((reconstructed_record .- record).^2)\n",
    "        push!(reconstruction_errors, error)\n",
    "    end\n",
    "    \n",
    "    return(reconstruction_errors)\n",
    "    \n",
    "end    # calc_reconstruction_errors()\n",
    "\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "# Define your refined autoencoder model as you have it\n",
    "refined_model = Chain(\n",
    "    Dense(4608, 256, relu),\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 32, relu),\n",
    "    Dense(32, 128, relu),\n",
    "    Dense(128, 256, relu),\n",
    "    Dense(256, 4608)\n",
    ")\n",
    "\n",
    "# Concatenate and normalize the training data\n",
    "training_data_combined = hcat(training_data_good, training_data_bad)\n",
    "training_data_normalized = min_max_normalize_matrix(training_data_combined)\n",
    "training_data_float32 = Float32.(training_data_normalized)\n",
    "\n",
    "@time begin\n",
    "\n",
    "    # Train the model\n",
    "    loss(x) = Flux.mse(refined_model(x), x)\n",
    "    opt = Adam()\n",
    "    \n",
    "    data = Iterators.repeated((training_data_float32,), 100)\n",
    "    Flux.train!(loss, Flux.params(refined_model), data, opt)\n",
    "    println(\"Model training complete.\")\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Sockets\n",
    "\n",
    "#global X_data = Matrix{Float32}(undef, 0, 0)\n",
    "\n",
    "hostname = gethostname()\n",
    "println(\"The name of the computer is: \", hostname)\n",
    "\n",
    "if hostname == \"QUEENSLAND-BASIN\"\n",
    "    \n",
    "    display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "    \n",
    "else\n",
    "    \n",
    "    display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "REC_LENGTH = 4608       # Number of WSE's in a Mk4 30-minute record\n",
    "SAMPLE_FREQUENCY = 2.56 # Mk4 sample frequency in Hertz\n",
    "SAMPLE_LENGTH = 1800    # record length in seconds\n",
    "SAMPLE_RATE = Float64(1/SAMPLE_FREQUENCY) # sample spacing in seconds\n",
    "\n",
    "#########################################################################################################################\n",
    "##    confidence_interval = 2.576  # corresponds to a 99% confidence interval (for a normal distribution)\n",
    "##    confidence_interval = 3.0    # corresponds to a 99.73% confidence interval (for a normal distribution)    \n",
    "##    confidence_interval = 3.29   # corresponds to a 99.9% confidence interval (for a normal distribution)\n",
    "#########################################################################################################################\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "initial_path = \"F:\\\\Card Data\\\\\"\n",
    "infil = pick_file(initial_path)\n",
    "\n",
    "f23_df, Data = get_hex_array(infil)\n",
    "\n",
    "f23_df = get_matches(Data, f23_df)\n",
    "\n",
    "# remove those vectors from F23 df that are not located in the Data vector df\n",
    "f23_df = f23_first_row_check(f23_df)\n",
    "\n",
    "X_data, X_date = get_heave(Data, f23_df);\n",
    "\n",
    "X_data = Float32.(X_data)\n",
    "\n",
    "println(string(length(X_date)),\" records processed.\\n\")\n",
    "println(\"\\nNow plot heave for each record to see suspect data!\")\n",
    "flush(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0dd36-420f-4d65-b334-e1da8b519279",
   "metadata": {},
   "source": [
    "### Run the hybrid model against data in the selected .BVA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720b056-4f92-46c0-b7a1-6f82fbabfabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction errors for good and bad training data separately\n",
    "errors_good = calc_reconstruction_errors(Float32.(min_max_normalize_matrix(training_data_good)), refined_model)\n",
    "errors_bad = calc_reconstruction_errors(Float32.(min_max_normalize_matrix(training_data_bad)), refined_model)\n",
    "\n",
    "# Weighting errors for bad data\n",
    "bad_weight_factor = 2.0  # Apply more weight to bad data\n",
    "\n",
    "# Weighted reconstruction errors\n",
    "weighted_errors_bad = errors_bad .* bad_weight_factor\n",
    "\n",
    "# Set separate thresholds\n",
    "good_threshold = quantile(errors_good, 0.95)  \n",
    "bad_threshold = quantile(weighted_errors_bad, 0.99)\n",
    "\n",
    "println(\"Good data threshold:\", good_threshold)\n",
    "println(\"Bad data threshold:\", bad_threshold)\n",
    "\n",
    "# Normalize new data\n",
    "X_data_32 = Float32.(X_data)\n",
    "mean_train_selected = mean(training_data_good, dims=1)[:, 1:size(X_data)[2]]\n",
    "std_train_selected = std(training_data_good, dims=1)[:, 1:size(X_data)[2]]\n",
    "X_new_normalized = Float32.((X_data_32 .- mean_train_selected) ./ std_train_selected)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "#==\n",
    "Note: The reconstruction error is the difference between the original data and the reconstructed data. \n",
    "      This error reflects how well the model can replicate the original data. \n",
    "      A low reconstruction error suggests the model has captured the structure of the data well, \n",
    "      while a high reconstruction error suggests that the data is unusual or anomalous.\n",
    "==#\n",
    "predicted_X_data = refined_model(X_new_normalized)\n",
    "reconstruction_error = sum((X_new_normalized .- predicted_X_data) .^ 2, dims=1)\n",
    "reconstruction_error_vector = vec(reconstruction_error)    # convert reconstruction_error matrix to vector\n",
    "\n",
    "# Normalize reconstruction errors to match scaling with good and bad thresholds\n",
    "normalized_reconstruction_error = min_max_normalize_matrix(reconstruction_error_vector)\n",
    "\n",
    "threshold = mean(normalized_reconstruction_error) + 3 * std(normalized_reconstruction_error)\n",
    "outliers = findall(normalized_reconstruction_error .> threshold)\n",
    "outlier_indices = [idx for idx in outliers]\n",
    "\n",
    "# Apply dual-threshold for outlier detection\n",
    "#outliers = findall(reconstruction_error_vector .> bad_threshold)\n",
    "uncertain = findall(x -> good_threshold < x <= bad_threshold, normalized_reconstruction_error)\n",
    "#==\n",
    "Note: \"uncertain\" generally refers to data points that the model finds difficult to classify as \n",
    "      either clearly \"inliers\" (normal data) or \"outliers\" (abnormal data). \n",
    "      These uncertain points fall in a \"gray zone,\" meaning they don't strongly resemble typical \n",
    "      inliers but also don't fully meet the model's criteria for outliers.\n",
    "==#\n",
    "# Get dates for outliers and uncertain data points\n",
    "outlier_dates = X_date[outlier_indices]\n",
    "uncertain_dates = X_date[[idx for idx in uncertain]]\n",
    "\n",
    "# Print out results\n",
    "if !isempty(outlier_dates)\n",
    "    println(\"Outliers detected at the following dates:\")\n",
    "    for date in outlier_dates\n",
    "        println(\"    \", Dates.format(date, \"yyyy-mm-dd HH:MM\"))\n",
    "    end\n",
    "else\n",
    "    println(\"No outliers detected.\")\n",
    "end\n",
    "\n",
    "if !isempty(uncertain_dates)\n",
    "    println(\"Uncertain data points detected at the following dates:\")\n",
    "    for date in uncertain_dates\n",
    "        println(\"    \", Dates.format(date, \"yyyy-mm-dd HH:MM\"))\n",
    "    end\n",
    "else\n",
    "    println(\"No uncertain data points detected.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92648524-1af9-45e1-8b7e-49e3ddfc3352",
   "metadata": {},
   "source": [
    "### Description of Model using a hybrid approach"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c677d8c-5355-45ce-8ade-902b787dcd36",
   "metadata": {},
   "source": [
    "This model consists of a fully connected neural network (often referred to as a dense network) with multiple layers. \n",
    "\n",
    "Here's a description of each part:\n",
    "\n",
    "1. Input Layer: Dense(4608, 256, relu)\n",
    "\n",
    "Input Size: The first layer has 4608 input units. This corresponds to the size of each data record (likely each water surface elevation record you are using, i.e., 4608 data points).\n",
    "\n",
    "Output Size: The output size of this layer is 256. So, the input data is projected down to a 256-dimensional space.\n",
    "\n",
    "Activation Function: The ReLU (Rectified Linear Unit) activation function is used here. It helps introduce non-linearity and ensures that the network can learn more complex representations of the data. ReLU outputs zero for any negative input and the input itself for positive values.\n",
    "\n",
    "2. Hidden Layer 1: Dense(256, 128, relu)\n",
    "\n",
    "Input Size: 256 (from the previous layer).\n",
    "\n",
    "Output Size: 128. This reduces the dimensionality further.\n",
    "\n",
    "Activation: Again, ReLU is used.\n",
    "\n",
    "3. Hidden Layer 2: Dense(128, 64, relu)\n",
    "\n",
    "Input Size: 128 (from the previous layer).\n",
    "\n",
    "Output Size: 64. The dimensionality is reduced again.\n",
    "\n",
    "Activation: ReLU.\n",
    "\n",
    "4. Hidden Layer 3: Dense(64, 128, relu)\n",
    "\n",
    "Input Size: 64 (from the previous layer).\n",
    "\n",
    "Output Size: 128. This layer begins expanding the dimensionality again.\n",
    "\n",
    "Activation: ReLU.\n",
    "\n",
    "5. Hidden Layer 4: Dense(128, 256, relu)\n",
    "\n",
    "Input Size: 128.\n",
    "\n",
    "Output Size: 256. The dimensionality continues to expand.\n",
    "\n",
    "Activation: ReLU.\n",
    "\n",
    "6. Output Layer: Dense(256, 4608)\n",
    "\n",
    "Input Size: 256 (from the previous layer).\n",
    "\n",
    "Output Size: 4608. This layer is intended to reconstruct the original input data. So, the network is mapping from the 256-dimensional space back to the original 4608-dimensional space (matching the original input data size).\n",
    "\n",
    "Activation: There’s no activation function here, as the goal of the output layer is to output raw values that will be used to calculate the reconstruction error. This means the output will be a direct reconstruction of the input data, without any nonlinear transformations.\n",
    "\n",
    "Key Characteristics of the Model:\n",
    "\n",
    "Autoencoder Architecture: This model is a type of autoencoder, which is used for unsupervised learning to learn a compressed representation (encoding) of the input data. The network is trained to reconstruct the input data from this compressed representation. Autoencoders typically consist of an encoder (which compresses the input data) and a decoder (which reconstructs the input data from the compressed form).\n",
    "\n",
    "    In this case, the encoder is the first half of the network (with decreasing dimensionality), and the decoder is the second half (with increasing dimensionality).\n",
    "\n",
    "Symmetry in Architecture: The architecture is symmetric, meaning the number of units decreases until the middle of the network (64 units) and then increases back to the original input size (4608 units). This symmetry is common in autoencoders and helps the model learn useful low-dimensional representations.\n",
    "\n",
    "ReLU Activation: The ReLU activation function is used throughout the model. ReLU is very popular in neural networks because it is computationally efficient and helps mitigate the vanishing gradient problem that can occur with other activation functions like sigmoid or tanh.\n",
    "\n",
    "Dimensionality Reduction and Reconstruction: The key idea behind using this model is to force the model to learn a compressed representation of the input data in the bottleneck layer (the 64-dimensional layer). The network learns to compress and then reconstruct the data, which helps it capture the most important features of the data. This makes it useful for anomaly detection, as the model will reconstruct normal data well but fail to accurately reconstruct outliers or anomalous data.\n",
    "\n",
    "Possible Adjustments/Considerations:\n",
    "\n",
    "Layer Sizes: The number of neurons in each layer determines the capacity of the network to learn complex representations. We could try experimenting with the size of each layer to see if it affects the model's performance. For example, if we want the model to learn more complex patterns, we could increase the number of neurons in the hidden layers.\n",
    "\n",
    "Activation Function: While ReLU is the most common activation function, in some cases, we could experiment with other activations like LeakyReLU or ELU (Exponential Linear Unit), especially if there are issues with neurons \"dying\" (becoming inactive) during training.\n",
    "\n",
    "Optimization: The model is using the Adam optimizer, which is an excellent choice for many machine learning tasks due to its adaptive learning rate. We could experiment with the learning rate to see if a different value speeds up or improves convergence.\n",
    "\n",
    "Training Data Size: If you have a large dataset, it may be necessary to adjust the model's capacity (e.g., increase layer sizes or change the number of layers) or experiment with mini-batch training instead of feeding the whole dataset at once.\n",
    "\n",
    "In summary, the refined model seems to be well-structured for anomaly detection based on the way it learns to reconstruct the input data. The use of ReLU activations and the symmetric architecture with a bottleneck layer at 64 units is a common and effective design for such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e28e8-0e6d-46d0-bfbd-4a9fa3ed3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(normalized_reconstruction_error, size=(1200,600))\n",
    "hline!([threshold], label=\"Threshold\")\n",
    "hline!([good_threshold], label=\"Good Threshold\", ls=:dot)\n",
    "hline!([bad_threshold], label=\"Bad Threshold\", ls=:dash)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef361b-abe7-4134-89ac-32a620ce504d",
   "metadata": {},
   "source": [
    "### Build the Model using a hybrid approach with overlapping segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb388d-05c6-4e7f-aea1-a1802aded80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "\n",
    "# (Keep existing functions: min_max_normalize_matrix, z_score_normalize_matrix, pad_or_truncate, get_heave)\n",
    "\n",
    "# Function to create overlapping segments for training/testing data\n",
    "function create_overlapping_segments(data, segment_length::Int, overlap::Int)\n",
    "############################################################################################\n",
    "    \n",
    "    step = segment_length - overlap\n",
    "    num_segments = max(1, (length(data) - segment_length) ÷ step + 1)\n",
    "    segments = [data[i:i + segment_length - 1] for i in 1:step:length(data)-segment_length+1]\n",
    "  \n",
    "    return(hcat(segments...))\n",
    "    \n",
    "end    # create_overlapping_segments()\n",
    "\n",
    "\n",
    "function calc_reconstruction_errors(data_matrix, model)\n",
    "#######################################################\n",
    "    \n",
    "    reconstruction_errors = Float32[]\n",
    "    for record in eachcol(data_matrix)\n",
    "        reconstructed_record = model(record)\n",
    "        error = mean((reconstructed_record .- record).^2)\n",
    "        push!(reconstruction_errors, error)\n",
    "    end\n",
    "    \n",
    "    return(reconstruction_errors)\n",
    "    \n",
    "end    # calc_reconstruction_errors()\n",
    "\n",
    "\n",
    "# Define your refined autoencoder model as you have it\n",
    "refined_model = Chain(\n",
    "    Dense(4608, 256, relu),\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 64, relu),\n",
    "    Dense(64, 128, relu),\n",
    "    Dense(128, 256, relu),\n",
    "    Dense(256, 4608)\n",
    ")\n",
    "\n",
    "# Generate new training data with 512-length segments\n",
    "segment_length = 512  # Update segment length to match the new model input size\n",
    "overlap = 256         # Set overlap as needed for your task\n",
    "\n",
    "# Apply this to both good and bad data\n",
    "training_data_good_segments = create_overlapping_segments(training_data_good, segment_length, overlap)\n",
    "training_data_bad_segments = create_overlapping_segments(training_data_bad, segment_length, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb451e-9882-4420-a9db-e296eb88e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function concatenate_segments(segments, group_size::Int)\n",
    "    num_segments = size(segments, 2) ÷ group_size\n",
    "    concatenated = [hcat(segments[:, i*group_size+1:(i+1)*group_size]...) for i in 0:num_segments-1]\n",
    "    return hcat(concatenated...)\n",
    "end\n",
    "\n",
    "# Apply to training data\n",
    "training_data_segments = hcat(training_data_good_segments, training_data_bad_segments)\n",
    "training_data_concatenated = concatenate_segments(training_data_segments, 9)\n",
    "\n",
    "# Normalize the segmented data\n",
    "training_data_segments_normalized = min_max_normalize_matrix(training_data_concatenated)\n",
    "\n",
    "# Convert to Float32\n",
    "training_data_segments_float32 = Float32.(training_data_segments_normalized)\n",
    "\n",
    "# Train the model\n",
    "loss(x) = Flux.mse(refined_model(x), x)\n",
    "opt = Adam()\n",
    "data = Iterators.repeated((training_data_segments_float32,), 100)\n",
    "# Reshape data to have 4608 features per sample (columns) and as many samples as possible (rows)\n",
    "data = reshape(data, 4608, :)\n",
    "\n",
    "Flux.train!(loss, Flux.params(refined_model), data, opt)\n",
    "println(\"Model training complete.\")\n",
    "\n",
    "# Calculate reconstruction errors for good and bad training data separately\n",
    "errors_good = calc_reconstruction_errors(Float32.(min_max_normalize_matrix(training_data_good_segments)), refined_model)\n",
    "errors_bad = calc_reconstruction_errors(Float32.(min_max_normalize_matrix(training_data_bad_segments)), refined_model)\n",
    "\n",
    "# Set separate thresholds\n",
    "good_threshold = quantile(errors_good, 0.99)  # e.g., 95th percentile of good errors\n",
    "bad_threshold = quantile(errors_bad, 0.995)    # e.g., 95th percentile of bad errors\n",
    "\n",
    "println(\"Good data threshold:\", good_threshold)\n",
    "println(\"Bad data threshold:\", bad_threshold)\n",
    "\n",
    "# Normalize new data\n",
    "X_data_32 = Float32.(X_data)\n",
    "mean_train_selected = mean(training_data_good_segments, dims=1)[:, 1:size(X_data)[2]]\n",
    "std_train_selected = std(training_data_good_segments, dims=1)[:, 1:size(X_data)[2]]\n",
    "X_new_normalized = Float32.((X_data_32 .- mean_train_selected) ./ std_train_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb8e50-14db-4323-ad73-1441f1ea86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the trained model\n",
    "##predicted_X_data = refined_model(X_new_normalized)\n",
    "function segment_data(data_matrix, segment_length)\n",
    "    n_segments = div(size(data_matrix, 1), segment_length)\n",
    "    return reshape(data_matrix[1:(n_segments * segment_length), :], segment_length, :)\n",
    "end\n",
    "    \n",
    "X_new_segmented = segment_data(X_new_normalized, 512)\n",
    "\n",
    "predicted_X_data = refined_model(X_new_segmented)\n",
    "\n",
    "reconstruction_error = sum((X_new_segmented .- predicted_X_data) .^ 2, dims=1)\n",
    "reconstruction_error_vector = vec(reconstruction_error)\n",
    "\n",
    "threshold = mean(reconstruction_error_vector) + 3 * std(reconstruction_error_vector)\n",
    "outliers = findall(reconstruction_error_vector .> threshold)\n",
    "\n",
    "# Apply dual-threshold for outlier detection\n",
    "outliers = findall(reconstruction_error_vector .> bad_threshold)\n",
    "uncertain = findall(x -> good_threshold < x <= bad_threshold, reconstruction_error_vector)\n",
    "\n",
    "# Get dates for outliers and uncertain data points\n",
    "outlier_dates = X_date[outliers]\n",
    "uncertain_dates = X_date[uncertain]\n",
    "\n",
    "# Print out results\n",
    "if !isempty(outlier_dates)\n",
    "    println(\"Outliers detected at the following dates:\")\n",
    "    for date in outlier_dates\n",
    "        println(\"    \", Dates.format(date, \"yyyy-mm-dd HH:MM\"))\n",
    "    end\n",
    "else\n",
    "    println(\"No outliers detected.\")\n",
    "end\n",
    "\n",
    "if !isempty(uncertain_dates)\n",
    "    println(\"Uncertain data points detected at the following dates:\")\n",
    "    for date in uncertain_dates\n",
    "        println(\"    \", Dates.format(date, \"yyyy-mm-dd HH:MM\"))\n",
    "    end\n",
    "else\n",
    "    println(\"No uncertain data points detected.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9ffd3-4b18-49f1-8088-ffff48e2597a",
   "metadata": {},
   "source": [
    "### Plot records with suspect data (as identified by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a900e-c753-4111-a30b-342afb8f8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for dynamic threshold based on wave amplitudes\n",
    "function dynamic_z_score_threshold(heave, base_threshold=3.0, k=0.5)\n",
    "    ####################################################################\n",
    "    \n",
    "    # Calculate the amplitude of heave\n",
    "    amplitude = abs.(heave)\n",
    "    \n",
    "    # Calculate mean and standard deviation of the amplitudes\n",
    "    mean_amplitude = mean(amplitude)\n",
    "    std_amplitude = std(amplitude)\n",
    "    \n",
    "    # Calculate the range of amplitudes\n",
    "    range_amplitude = maximum(amplitude) - minimum(amplitude)\n",
    "    \n",
    "    # Calculate dynamic threshold\n",
    "    dynamic_threshold = base_threshold * (1 + k * (range_amplitude / std_amplitude))\n",
    "    \n",
    "    # Cap the dynamic threshold\n",
    "    max_threshold = 3.99  # or any other maximum threshold you prefer\n",
    "    dynamic_threshold = min(dynamic_threshold, max_threshold)\n",
    "    \n",
    "    return dynamic_threshold\n",
    "    \n",
    "end  # dynamic_z_score_threshold()\n",
    "\n",
    "\n",
    "for ii ∈ outlier_indices\n",
    "\n",
    "    # Initialize variables\n",
    "    start_time = X_date[ii]\n",
    "    global heave = X_data[:, ii]\n",
    "    end_time = start_time + Minute(30)\n",
    "    xvals = start_time + Microsecond.((0:REC_LENGTH-1) / SAMPLE_FREQUENCY * 1000000)\n",
    "\n",
    "    # Plot initialization\n",
    "    p1 = plot(size=(2000, 400), dpi=100, framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "        legend=:topright, xtickfont=font(8), ytickfont=font(8), bottommargin = 10Plots.mm,\n",
    "        grid=true, gridlinewidth=0.125, gridstyle=:dot, gridalpha=1)\n",
    "    \n",
    "    tm_tick = range(start_time, end_time, step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick, \"MM\")\n",
    "    \n",
    "    # Calculate dynamic confidence interval\n",
    "    global confidence_interval = dynamic_z_score_threshold(heave)\n",
    "\n",
    "    # Identify outliers using modified z-score\n",
    "    z_score_indices, mod_z_scores = modified_z_score(heave, confidence_interval)\n",
    "    if !isempty(z_score_indices)\n",
    "        scatter!(p1, xvals[z_score_indices], heave[z_score_indices], \n",
    "            markersize=4, markerstrokecolor=:red, markerstrokewidth=1, \n",
    "            markercolor=:white, markershape=:utriangle, label=\"\")\n",
    "    end\n",
    "\n",
    "    # Plot confidence limits\n",
    "    confidence_limits = calc_confidence_limits(heave, confidence_interval)\n",
    "    hline!(p1, [confidence_limits[1], confidence_limits[2]], color=:red, lw=1, linestyle=:dash, label=\"\")\n",
    "\n",
    "    # Plot heave data\n",
    "    plot!(p1, xvals, heave, xlims=(xvals[1], xvals[end]), lw=0.5, lc=:blue, alpha=0.5, \n",
    "        xticks=(tm_tick, ticks), label=\"\")\n",
    "\n",
    "    # Annotate plot with the number of outliers and confidence interval\n",
    "    num_outliers = length(z_score_indices)\n",
    "    suspect_string = string(\"  \", string(ii),\" \",Dates.format(start_time, \"yyyy-mm-dd HH:MM\"), \" - \", num_outliers, \" Possible outliers\") # using Confidence Interval of \", \n",
    "##        @sprintf(\"%.2f\", confidence_interval))\n",
    "    annotate!(p1, xvals[1], maximum(heave) * 0.9, text(suspect_string, :left, 10, :blue))\n",
    "\n",
    "    display(p1)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc708bf-c8c4-420a-aa7a-b4469e30fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "function modified_z_score(data, threshold)\n",
    "##########################################\n",
    "    \n",
    "    med = median(data)\n",
    "    mad = median(abs.(data .- med))\n",
    "    mod_z_scores = 0.6745 * (data .- med) ./ mad\n",
    "    outlier_indices = findall(x -> abs(x) > threshold, mod_z_scores)\n",
    "    \n",
    "    return(outlier_indices, mod_z_scores)\n",
    "    \n",
    "end    # modified_z_score()\n",
    "\n",
    "heave = X_data[:,1]\n",
    "# Calculate dynamic confidence interval\n",
    "    confidence_interval = dynamic_z_score_threshold(heave)\n",
    "\n",
    "    # Identify outliers using modified z-score\n",
    "    outlier_indices, mod_z_scores = modified_z_score(heave, confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bb27f-46ce-4ba4-9298-ba8d60172f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_z_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fb44a-4539-45c8-947b-1f8c5ddbf1f0",
   "metadata": {},
   "source": [
    "### Save separated data to file (Note: does not include Model data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6edf38-ae82-406e-be5d-c78e23320cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Dates\n",
    "\n",
    "outfil = \"BVA_trainng_data_\"*Dates.format(now(), \"yyyy_mm_dd_HHMM\")*\".JLD2\" \n",
    "# Save all relevant data and labels\n",
    "@save outfil training_data_good training_data_bad\n",
    "println(\"Data and labels saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9f34b-f846-4ad3-80fb-95e77c5de538",
   "metadata": {},
   "source": [
    "### Recover earlier separated data from file (Note: does not include Model data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43472336-e065-47bd-86c1-0e1b3c6ef28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "\n",
    "using NativeFileDialog: pick_file\n",
    "\n",
    "# Load the model and optimizer states from the JLD2 file\n",
    "infil = pick_file()\n",
    "\n",
    "# Load all saved data and labels\n",
    "##@load infil training_data training_date training_data_good training_date_good training_data_bad training_date_bad training_indicies_good training_indicies_bad training_labels # median_train std_train\n",
    "@load infil training_data_good training_data_bad training_data_bad\n",
    "\n",
    "println(\"Data and labels loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = Y_train \n",
    "training_date = Y_date \n",
    "training_data_good = Y_train_good\n",
    "training_date_good = Y_date_good \n",
    "training_data_bad = Y_train_bad \n",
    "training_date_bad = Y_date_bad \n",
    "training_indicies_good = good_indices\n",
    "training_indicies_bad = bad_indices \n",
    "training_labels = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d9aac-55ae-42ce-97a5-c116a98e32a2",
   "metadata": {},
   "source": [
    "### Recover earlier separated Model data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2ccc1-7f57-4387-8d53-3829998623b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Flux\n",
    "using NativeFileDialog: pick_file\n",
    "\n",
    "# Load the model and optimizer states from the JLD2 file\n",
    "infil = pick_file()\n",
    "\n",
    "# Load model state, optimizer, and data/labels\n",
    "loaded_model_state, opt_state, X_train, X_date, X_train_good, X_date_good, X_train_bad, X_date_bad, labels = \n",
    "    JLD2.@load infil model_state opt_state X_train X_date X_train_good X_date_good X_train_bad X_date_bad labels\n",
    "\n",
    "# Reconstruct the model\n",
    "model = Chain(\n",
    "    Dense(4608, 128, relu),  # Encoder\n",
    "    Dense(128, 64, relu),    # Bottleneck\n",
    "    Dense(64, 128, relu),    # Decoder\n",
    "    Dense(128, 4608)         # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "# Load the model parameters\n",
    "Flux.loadmodel!(model, model_state)\n",
    "\n",
    "println(\"Model, optimizer, data, and labels loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b66f2-a001-4bcc-be8e-cffa970a7e87",
   "metadata": {},
   "source": [
    "### Append new data to older data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3abbc78-ce45-4786-b965-69538efe5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append new data\n",
    "X_train = hcat(X_train, new_X_train)\n",
    "X_date = vcat(X_date, new_X_date)\n",
    "\n",
    "# Append new labels if they exist\n",
    "labels_good = vcat(labels_good, new_labels_good)  # or update as needed\n",
    "labels_bad = vcat(labels_bad, new_labels_bad)      # or update as needed\n",
    "\n",
    "# Save the updated data and labels\n",
    "@save \"processed_data_with_labels.jld2\" X_train X_date X_train_good X_date_good X_train_bad X_date_bad selected_indices labels\n",
    "println(\"Updated data and labels saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83689e74-d25d-4b57-9b74-95df95ad8284",
   "metadata": {},
   "source": [
    "### Save model, optimiser, and heave data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4771d-1ad5-43a9-bc3a-e367ec84b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, JLD2, Dates\n",
    "\n",
    "# Save the model state, optimizer state, and relevant data/labels\n",
    "model_state = Flux.state(model)  # Get the model's parameters\n",
    "opt = opt # Flux.setup(Adam(), model)\n",
    "\n",
    "outfil = \"BVA_model_and_data_\"*Dates.format(now(), \"yyyy_mm_dd_HHMM\")*\".JLD2\" \n",
    "\n",
    "# Save model and optimizer and normalised wave data to a JLD2 file\n",
    "@save outfil model_state opt X_train X_date X_train_good X_date_good X_train_bad X_date_bad labels\n",
    "\n",
    "println(\"Data and model saved successfully to \",outfil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fce6a-aaa8-4d0f-aeb2-6491fc910855",
   "metadata": {},
   "source": [
    "### Recover model, optimiser, and heave data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07050671-bc78-4328-98f0-09e08ad18494",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Flux\n",
    "using NativeFileDialog: pick_file\n",
    "\n",
    "# Define your model architecture here (example)\n",
    "function create_model()\n",
    "    return Chain(\n",
    "        Dense(10, 5, relu),\n",
    "        Dense(5, 1)\n",
    "    )\n",
    "end\n",
    "\n",
    "\n",
    "# Load the model and optimizer states from the JLD2 file\n",
    "infil = pick_file()\n",
    "\n",
    "# Change the file extension to uppercase\n",
    "infil = replace(infil, \".jld2\" => \".JLD2\")\n",
    "\n",
    "println(\"File selected: $infil\")  # Print the selected file path\n",
    "\n",
    "try\n",
    "    # Load model state, optimizer, and data/labels\n",
    "    model_state, opt_state, X_train, X_date, X_train_good, X_date_good, X_train_bad, X_date_bad, labels = \n",
    "        @load infil model_state opt_state X_train X_date X_train_good X_date_good X_train_bad X_date_bad labels\n",
    "\n",
    "    # Reconstruct the model from the architecture\n",
    "    model = create_model()  # Call the function to create the model\n",
    "    Flux.load!(model, model_state)  # Load the model parameters\n",
    "\n",
    "    # Set up the optimizer\n",
    "    opt = Flux.setup(Adam(), model)  # Reinitialize the optimizer with the model\n",
    "    Flux.load!(opt, opt_state)  # Load the optimizer state\n",
    "\n",
    "    println(\"Model, optimizer, and data loaded successfully.\")\n",
    "catch e\n",
    "    println(\"Error loading data: \", e)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467e1f1-d570-48b8-820d-2adbc5e19faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, JLD2\n",
    "\n",
    "# Load the model state, optimizer state, and relevant data/labels\n",
    "model_state, opt_state, X_train, X_date, X_train_good, X_date_good, X_train_bad, X_date_bad, labels = @load outfil\n",
    "\n",
    "# Reconstruct the model and optimizer from the loaded states\n",
    "model = Flux.loadmodel(model_state)  # Use your model architecture to reconstruct it\n",
    "opt = Flux.setup(Adam(), model)  # Reinitialize the optimizer with the model\n",
    "\n",
    "println(\"Model, optimizer, and data loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd91347-f2d7-4a35-b9ab-4f4d03ab914b",
   "metadata": {},
   "source": [
    "### Select records that will NOT be uploaded to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422400d6-e8d6-4d13-a96d-ea4896b02596",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tk\n",
    "\n",
    "# Make a copy of X_train in case want to roll-back\n",
    "X_train_old = X_train\n",
    "\n",
    "# Convert DateTimes into strings\n",
    "all_dates = Dates.format.(X_date, \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "# Initialize selection window\n",
    "w = Toplevel(\"Select Date\", 235, 400)\n",
    "tcl(\"pack\", \"propagate\", w, false)\n",
    "f = Frame(w)\n",
    "pack(f, expand=true, fill=\"both\")\n",
    "\n",
    "f1 = Frame(f)\n",
    "lb = Treeview(f1, all_dates)\n",
    "scrollbars_add(f1, lb)\n",
    "pack(f1, expand=true, fill=\"both\")\n",
    "\n",
    "# Style button\n",
    "tcl(\"ttk::style\", \"configure\", \"TButton\", foreground=\"blue\", font=\"arial 16 bold\")\n",
    "b = Button(f, \"Ok\")\n",
    "pack(b)\n",
    "\n",
    "# Global array to store selected dates\n",
    "global bad_array = []\n",
    "\n",
    "# Collect dates when the button is pressed\n",
    "bind(b, \"command\") do path\n",
    "    file_choice = get_value(lb)\n",
    "    if !isempty(file_choice)\n",
    "        push!(bad_array, file_choice[1])\n",
    "        println(\"Added to removal list: \", file_choice[1])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to close window and process columns\n",
    "function finalize_selection(args...)\n",
    "************************************\n",
    "    \n",
    "    destroy(w)  # Close Tk window\n",
    "\n",
    "    # Find indices of bad_dates in X_date\n",
    "    bad_cols = findall(x -> x in bad_array, all_dates)\n",
    "\n",
    "    # Remove columns from X_train based on `bad_cols`\n",
    "    global X_train = X_train[:, setdiff(1:size(X_train, 2), bad_cols)]\n",
    "    println(\"Removed columns based on selection.\")\n",
    "    \n",
    "end    # finalize_selection()\n",
    "\n",
    "# Bind finalize function to window close event\n",
    "tcl(\"wm\", \"protocol\", w, \"WM_DELETE_WINDOW\", finalize_selection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ae92d-f4e5-4b9b-8342-4304e121387e",
   "metadata": {},
   "source": [
    "### Initial Code for Training an Autoencoder in Julia (using Flux.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c254d-570b-41bf-b65d-37c15f4d48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "\n",
    "function min_max_normalize_matrix(X)\n",
    "    min_vals = minimum(X, dims=1)  # Compute min for each column\n",
    "    max_vals = maximum(X, dims=1)  # Compute max for each column\n",
    "    return (X .- min_vals) ./ (max_vals .- min_vals)\n",
    "end\n",
    "\n",
    "\n",
    "function z_score_normalize_matrix(X)\n",
    "    mean_vals = mean(X, dims=1)  # Mean for each column\n",
    "    std_vals = std(X, dims=1)    # Standard deviation for each column\n",
    "    return (X .- mean_vals) ./ std_vals\n",
    "end\n",
    "\n",
    "\n",
    "function pad_or_truncate(record, target_length=4608)\n",
    "####################################################\n",
    "#==    \n",
    "    if length(record) < target_length\n",
    "        # Pad with zeros (or any other value you prefer)\n",
    "        return vcat(record, zeros(Float32, target_length - length(record)))\n",
    "    elseif length(record) > target_length\n",
    "        # Truncate to the target length\n",
    "        return record[1:target_length]\n",
    "    else\n",
    "        return record\n",
    "    end\n",
    "==#\n",
    "    length(record) < target_length ? vcat(record, zeros(Float32, target_length - length(record))) :\n",
    "                                     record[1:target_length]\n",
    "\n",
    "end    # pad_or_truncate()\n",
    "\n",
    "\n",
    "function get_heave(Data, f23_df)\n",
    "################################\n",
    "    \n",
    "    heave_array = []\n",
    "    X_date = []\n",
    "    \n",
    "    for idx in 1:nrow(f23_df)\n",
    "\n",
    "        if !isnothing(f23_df.Data_vector[idx])\n",
    "    \n",
    "            start_date, start_val, end_val = get_start_end_dates(f23_df,idx)\n",
    "            if start_val > 0\n",
    "                print(\".\")\n",
    "                heave, north, west = get_hnw(Data,start_val,end_val)\n",
    "\n",
    "                # ensure we have 4608 data points\n",
    "                push!(heave_array,pad_or_truncate(heave, 4608))\n",
    "                push!(X_date,start_date)\n",
    "            end\n",
    "\n",
    "        end\n",
    "    \n",
    "    end\n",
    "\n",
    "    return(hcat(heave_array...), X_date)\n",
    "\n",
    "end    # get_heave()\n",
    "\n",
    "\n",
    "function calc_reconstruction_errors(X_train_float32, model)\n",
    "####################################################\n",
    "    \n",
    "    reconstruction_errors = Float32[]\n",
    "    \n",
    "    for record in eachcol(X_train_float32)  # Each record is now a column with 14 features\n",
    "        reconstructed_record = model(record)  # Pass the record to the autoencoder\n",
    "        error = mean((reconstructed_record .- record).^2)  # Calculate the reconstruction error\n",
    "        push!(reconstruction_errors, error)  # Store the error\n",
    "    end\n",
    "    \n",
    "    return(reconstruction_errors)\n",
    "\n",
    "end    # calc_reconstruction_errors()\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "@time begin\n",
    "# Define autoencoder model\n",
    "model = Chain(\n",
    "    Dense(4608, 128, relu),  # Encoder\n",
    "    Dense(128, 64, relu),    # Bottleneck\n",
    "    Dense(64, 128, relu),    # Decoder\n",
    "    Dense(128, 4608)         # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "# Define the loss function (e.g., Mean Squared Error for reconstruction)\n",
    "loss(x) = Flux.mse(model(x), x)\n",
    "\n",
    "# Optimizer: Adam with default parameters (learning rate, etc.)\n",
    "opt = Adam()\n",
    "   \n",
    "X_train, X_date = get_heave(Data, f23_df)\n",
    "\n",
    "# Normalize your training data\n",
    "##X_train_normalized = normalize_records(X_train)\n",
    "X_train_normalized = min_max_normalize_matrix(X_train)\n",
    "    \n",
    "# Convert WSE data to Float32\n",
    "X_train_float32 = Float32.(X_train_normalized)\n",
    "\n",
    "# calculate the reconstruction_errors\n",
    "reconstruction_errors_model = calc_reconstruction_errors(X_train_float32, model)\n",
    "\n",
    "# Use the converted data for training\n",
    "data = Iterators.repeated((X_train_float32,), 100)  # Example of data iteration for training\n",
    "\n",
    "# Train the model\n",
    "Flux.train!(loss, Flux.params(model), data, opt)\n",
    "end    # @time\n",
    "\n",
    "println(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c742b1c-1339-4146-b65a-396201fb688b",
   "metadata": {},
   "source": [
    "### Save model, optimiser, and normalised heave data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e3f1c-d875-486f-97b6-3a1098d00c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, JLD2\n",
    "\n",
    "# Save the model and optimizer\n",
    "model_state = Flux.state(model)\n",
    "opt_state = opt # Flux.setup(Adam(), model)\n",
    "\n",
    "outfil = \"HVA_model_\"*Dates.format(now(), \"yyyy_mm_dd_HHMM\")*\".jld2\" \n",
    "\n",
    "# Save model and optimizer and normalised wave data to a JLD2 file\n",
    "jldsave(outfil; model_state, opt_state, X_train_float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48295e39-eb2b-44e1-a5fc-6cff215cf9a3",
   "metadata": {},
   "source": [
    "### Recover saved model, optimiser, and normalised heave data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e3a17-8520-4168-8b41-bba92c41515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Flux, Tk\n",
    "\n",
    "# Load the model and optimizer states from the JLD2 file\n",
    "infil = pick_file()\n",
    "loaded_data = jldopen(infil, \"r\") do file\n",
    "    old_model_state = file[\"model_state\"]  # Load model state\n",
    "    old_opt_state = file[\"opt_state\"]      # Load optimizer state\n",
    "    old_X_train_float32 = file[\"X_train_float32\"]  # Load the previous X_train_float32 data\n",
    "    return (old_model_state, old_opt_state, old_X_train_float32) # Return all states and data\n",
    "end\n",
    "\n",
    "old_model_state, old_opt_state, old_X_train_float32 = loaded_data\n",
    "\n",
    "# Define the old model architecture\n",
    "old_model = Chain(\n",
    "    Dense(4608, 128, relu),  # Encoder\n",
    "    Dense(128, 64, relu),    # Bottleneck\n",
    "    Dense(64, 128, relu),     # Decoder\n",
    "    Dense(128, 4608)          # Output layer, reconstructs input\n",
    ")\n",
    "\n",
    "# Restore model parameters from the loaded state\n",
    "for (layer, state) in zip(old_model.layers, old_model_state[:layers])\n",
    "    layer.weight .= state.weight   # Assign saved weights\n",
    "    layer.bias .= state.bias       # Assign saved biases\n",
    "end\n",
    "\n",
    "# Restore the optimizer state directly\n",
    "old_opt = old_opt_state;  # Assign the loaded optimizer state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc507551-eb1f-44e2-bdad-249b4a91a31c",
   "metadata": {},
   "source": [
    "### Append new data to existing model and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcb9dd-3844-43d9-980e-02fb6c01d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using JLD2\n",
    "\n",
    "infil = pick_file()\n",
    "\n",
    "# Load the existing model and optimizer\n",
    "loaded_model, opt = JLD2.@load infil model opt\n",
    "\n",
    "# Load new records and prepare the data\n",
    "new_X_train, _ = get_heave(Data, f23_df)  # Replace with your new data fetching function\n",
    "new_X_train_normalized = normalize_records(new_X_train)\n",
    "new_X_train_float32 = Float32.(new_X_train_normalized)\n",
    "\n",
    "# Combine with the previous training data (if applicable)\n",
    "# You can concatenate with previous training data if desired\n",
    "X_combined = hcat(old_X_train_float32, new_X_train_float32)\n",
    "\n",
    "# Define the loss function again\n",
    "loss(x) = Flux.mse(loaded_model(x), x)\n",
    "\n",
    "# Prepare the data for training (iterating over the new combined data)\n",
    "data = Iterators.repeated((X_combined,), 100)\n",
    "\n",
    "# Train the model on the new data\n",
    "Flux.train!(loss, Flux.params(loaded_model), data, opt)\n",
    "\n",
    "outfil = infil\n",
    "\n",
    "# Save model and optimizer and normalised wave data to a JLD2 file\n",
    "jldsave(outfil; model_state, opt_state, X_train_float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
